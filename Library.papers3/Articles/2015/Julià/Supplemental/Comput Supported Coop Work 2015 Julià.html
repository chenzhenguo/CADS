<!DOCTYPE HTML>
<html lang="en-gb" class="no-js">
    <head>
        <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=2.5,user-scalable=yes">
    <meta name="citation_publisher" content="Springer Netherlands"/>
    <meta name="citation_title" content="Towards Concurrent Multi-Tasking in Shareable Interfaces"/>
    <meta name="citation_doi" content="10.1007/s10606-015-9218-5"/>
    <meta name="citation_language" content="en"/>
    <meta name="citation_abstract_html_url" content="https://link.springer.com/article/10.1007/s10606-015-9218-5"/>
    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10606-015-9218-5"/>
    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007%2Fs10606-015-9218-5.pdf"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q&#x3D;doi:10.1007/s10606-015-9218-5&amp;api_key&#x3D;"/>
    <meta name="citation_firstpage" content="79"/>
    <meta name="citation_lastpage" content="108"/>
    <meta name="citation_author" content="Carles F. Julià"/>
    <meta name="citation_author_institution" content="Universitat Pompeu Fabra"/>
    <meta name="citation_author_email" content="carles.fernandez@upf.edu"/>
    <meta name="citation_author" content="Sergi Jordà"/>
    <meta name="citation_author_institution" content="Universitat Pompeu Fabra"/>
    <meta name="dc.identifier" content="10.1007/s10606-015-9218-5"/>
    <meta name="format-detection" content="telephone&#x3D;no"/>
    <meta name="meta:description" content="Shareable interfaces, those that can be interacted simultaneously by several users, are a common tool used both in CSCW research and in real world applications. They tend however to lack a capability "/>
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:title" content="Towards Concurrent Multi-Tasking in Shareable Interfaces"/>
    <meta name="twitter:description" content="Shareable interfaces, those that can be interacted simultaneously by several users, are a common tool used both in CSCW research and in real world applications. They tend however to lack a capability "/>
    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10606/24/2.jpg"/>
    <meta name="twitter:image:alt" content="Content cover image"/>
    <meta name="twitter:site" content="SpringerLink"/>
    <meta name="citation_journal_title" content="Computer Supported Cooperative Work (CSCW)"/>
    <meta name="citation_journal_abbrev" content="Comput Supported Coop Work"/>
    <meta name="citation_volume" content="24"/>
    <meta name="citation_issue" content="2-3"/>
    <meta name="citation_issn" content="0925-9724"/>
    <meta name="citation_issn" content="1573-7551"/>
    <meta name="citation_online_date" content="2015/03/28"/>
    <meta name="citation_cover_date" content="2015/06/01"/>
    <meta name="citation_article_type" content="Article"/>
    <meta property="og:title" content="Towards Concurrent Multi-Tasking in Shareable Interfaces"/>
    <meta property="og:description" content="Shareable interfaces, those that can be interacted simultaneously by several users, are a common tool used both in CSCW research and in real world applications. They tend however to lack a capability "/>
    <meta property="og:type" content="Article"/>
    <meta property="og:url" content="https://link.springer.com/article/10.1007/s10606-015-9218-5"/>
    <meta property="og:image" content="https://link.springer.com/springerlink-static/373808856/images/png/SL-Square.png"/>
    <meta property="og:site_name" content="SpringerLink"/>

        <title>Towards Concurrent Multi-Tasking in Shareable Interfaces | SpringerLink</title>
        <link rel="canonical" href="https://link.springer.com/article/10.1007/s10606-015-9218-5"/>
        <link rel="shortcut icon" href="/springerlink-static/373808856/images/favicon/favicon.ico" />
<link rel="icon" sizes="16x16 32x32 48x48" href="/springerlink-static/373808856/images/favicon/favicon.ico">
<link rel="icon" sizes="16x16" type="image/png" href="/springerlink-static/373808856/images/favicon/favicon-16x16.png">
<link rel="icon" sizes="32x32" type="image/png" href="/springerlink-static/373808856/images/favicon/favicon-32x32.png">
<link rel="icon" sizes="48x48" type="image/png" href="/springerlink-static/373808856/images/favicon/favicon-48x48.png">
<link rel="apple-touch-icon-precomposed" href="/springerlink-static/373808856/images/favicon/app-icon-iphone@3x.png">
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="/springerlink-static/373808856/images/favicon/ic_launcher_hdpi.png" />
<link rel="apple-touch-icon-precomposed" sizes="76x76" href="/springerlink-static/373808856/images/favicon/app-icon-ipad.png" />
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="/springerlink-static/373808856/images/favicon/app-icon-114x114.png" />
<link rel="apple-touch-icon-precomposed" sizes="120x120" href="/springerlink-static/373808856/images/favicon/app-icon-iphone@2x.png" />
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/springerlink-static/373808856/images/favicon/ic_launcher_xxhdpi.png" />
<link rel="apple-touch-icon-precomposed" sizes="152x152" href="/springerlink-static/373808856/images/favicon/app-icon-ipad@2x.png" />
<link rel="apple-touch-icon-precomposed" sizes="180x180" href="/springerlink-static/373808856/images/favicon/app-icon-iphone@3x.png" />
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/springerlink-static/373808856/images/favicon/ic_launcher_xxhdpi.png">
        <link rel="dns-prefetch" href="//fonts.gstatic.com" />
<link rel="dns-prefetch" href="//fonts.googleapis.com" />
<link rel="dns-prefetch" href="//google-analytics.com" />
<link rel="dns-prefetch" href="//www.google-analytics.com" />
<link rel="dns-prefetch" href="//www.googletagservices.com" />
<link rel="dns-prefetch" href="//www.googletagmanager.com" />
<link rel="dns-prefetch" href="//static-content.springer.com" />
        <link rel="stylesheet" href="/springerlink-static/373808856/css/basic.css" media="screen">
<link rel="stylesheet" href="/springerlink-static/373808856/css/styles.css" class="js-ctm" media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
<link rel="stylesheet" href="/springerlink-static/373808856/css/print.css" media="print">


            <script id="webtrekk-properties">
    var webtrekkProperties = {
          trackDomain : "springergmbh01.webtrekk.net",
          trackId : "935649882378213",
            bpJson: {1:"1600047252;3000133174;8200828607",2:"Carnegie Mellon University Hunt Library;Carnegie Mellon University Hunt Library;Center for Research LIbraries c/o NERL"}
      };
    </script>
    <script type="text/javascript" async src="/springerlink-static/373808856/js/webtrekk_v4.min.js"></script>

            <script>
        var dataLayer = [{
                'GA Key':'UA-26408784-1',
                'Event Category':'Article',
                'Open Access':'N',
                'Labs':'Y',
                'DOI':'10.1007\/s10606-015-9218-5',
                'VG Wort Identifier':'pw-vgzm.415900-10.1007-s10606-015-9218-5',
                'HasAccess':'Y',
                'Full HTML':'Y',
                'Has Body':'Y',
                'Static Hash':'373808856',
                'Has Preview':'N',
                'user':{'license': {'businessPartnerID': ['1600047252', '3000133174', '8200828607'], 'businessPartnerIDString': '1600047252|3000133174|8200828607'}},
                'content':{'type': 'article', 'category': {'pmc': {'primarySubject': 'Computer Science', 'primarySubjectCode': 'I', 'secondarySubjects': {'4': 'Interdisciplinary Studies', '5': 'Social Sciences, general', '1': 'Computer Science, general', '2': 'User Interfaces and Human Computer Interaction', '3': 'Psychology, general'}, 'secondarySubjectCodes': {'4': 'V23000', '5': 'X00000', '1': 'I00001', '2': 'I18067', '3': 'Y00007'}}}},
                'Access Type':'subscription',
                'Page':'article',
                'Bpids':'1600047252, 3000133174, 8200828607',
                'Bpnames':'Carnegie Mellon University Hunt Library, Carnegie Mellon University Hunt Library, Center for Research LIbraries c\/o NERL',
                'SubjectCodes':'SCI, SCI00001, SCI18067, SCY00007, SCV23000, SCX00000',
                'Keywords':'Concurrent interaction, Multi-user, Shareable interfaces, Multi-tasking, Agent exclusivity',
                'Country':'US',
                'Journal Id':'10606',
                'Journal Title':'Computer Supported Cooperative Work (CSCW)',
        }];
    </script>

    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
            'https://www.googletagmanager.com/gtm.js?cache-busting=' + new Date().getTime() + '&id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-WCF9Z9');</script>

    </head>
    <body>
        <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
                      height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    <nav class="skip-to">
    <a class="skip-to__link skip-to__link--article" href="#main-content" title="Skip directly to the main content">Skip to main content</a>
        <a class="skip-to__link skip-to__link--contents" href="#article-contents" title="Skip directly to section list">Skip to sections</a>
</nav>
        <div class="page-wrapper">
            <noscript>
    <div class="nojs-banner u-interface">
        <p>This service is more advanced with JavaScript available, learn more at <a
                href="http://activatejavascript.org" target="_blank" rel="noopener noreferrer">http://activatejavascript.org</a>
        </p>
    </div>
</noscript>
                    <div id="leaderboard" class="leaderboard u-hide" data-component="SpringerLink.GoogleAds" data-namespace="leaderboard"></div>

                <header id="header" class="header u-interface" role="banner">
        <div class="header__content">
            <div class="header__menu-container">
                    <a id="logo" class="site-logo" href="/" title="Go to homepage">
            <span class="u-screenreader-only">SpringerLink</span>
            <svg class="site-logo__springer" width="148" height="30" role="img" aria-label="SpringerLink Logo">
                <image width="148" height="30" alt="SpringerLink Logo" src="/springerlink-static/373808856/images/png/springerlink.png" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/springerlink-static/373808856/images/svg/springerlink.svg"></image>
            </svg>
    </a>

                    <div class="nav-container u-interface">
    <div class="global-nav__wrapper">
        <div class="search-button">
            <a class="search-button__label" href="#search-container">
                <span class="search-button__title">Search</span><svg width="12" height="12" viewBox="222 151 12 12" version="1.1" xmlns="http://www.w3.org/2000/svg">
                    <path d="M227 159C228.7 159 230 157.7 230 156 230 154.3 228.7 153 227 153 225.3 153 224 154.3 224 156 224 157.7 225.3 159 227 159L227 159 227 159 227 159ZM230 160.1L231.1 159 233.9 161.7C234.2 162.1 234.2 162.6 233.9 162.9 233.6 163.2 233.1 163.2 232.7 162.9L230 160.1 230 160.1 230 160.1 230 160.1ZM227 161L227 161C224.2 161 222 158.8 222 156 222 153.2 224.2 151 227 151 229.8 151 232 153.2 232 156 232 158.8 229.8 161 227 161L227 161 227 161 227 161 227 161Z" stroke="none" fill-rule="evenodd"></path>
                </svg>
            </a>
        </div>

        <ul class="global-nav" data-component="SpringerLink.Menu" data-title="Navigation menu" data-text="Menu" role="navigation">
            <li>
                <a href="/">
                    <span class="u-overflow-ellipsis">Home</span>
                </a>
            </li>
            <li>
                <a href="/contactus">
                    <span class="u-overflow-ellipsis">Contact Us</span>
                </a>
            </li>

                <li class="global-nav__logged-out">
                    <a class="test-login-link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%252Fs10606-015-9218-5">
                        <span class="u-overflow-ellipsis">Log in</span>
                    </a>
                </li>

        </ul>
    </div> 
</div> 
            </div>

        </div>

            <div id="search-container">
                <div class="search">
                    <div class="search__content">
                        <form class="u-form-single-input" action="/search" method="get" role="search">
    <input aria-label="Search" name="query" type="text" autocomplete="off" value="" placeholder="Search">
    <input class="u-hide-text" type="submit" value="Submit" title="Submit">
    <svg class="u-vertical-align-absolute" width="13" height="13" viewBox="222 151 13 13" version="1.1" xmlns="http://www.w3.org/2000/svg">
        <path d="M227 159C228.7 159 230 157.7 230 156 230 154.3 228.7 153 227 153 225.3 153 224 154.3 224 156 224 157.7 225.3 159 227 159L227 159 227 159 227 159ZM230 160.1L231.1 159 233.9 161.7C234.2 162.1 234.2 162.6 233.9 162.9 233.6 163.2 233.1 163.2 232.7 162.9L230 160.1 230 160.1 230 160.1 230 160.1ZM227 161L227 161C224.2 161 222 158.8 222 156 222 153.2 224.2 151 227 151 229.8 151 232 153.2 232 156 232 158.8 229.8 161 227 161L227 161 227 161 227 161 227 161Z" stroke="none" fill-rule="evenodd"/>
    </svg>
</form>
                    </div>
                </div>
            </div>
    </header>

            
            

            <main role="main" id="main-content">
                <article class="main-wrapper">
                    <div class="main-container uptodate-recommendations-off">
                        <aside class="main-sidebar-left">
                            <div class="main-sidebar-left__content">
                                <div class="test-cover cover-image">
        <a class="test-cover-link" href="/journal/10606" title="Computer Supported Cooperative Work (CSCW)">
            <img class="test-cover-image" src="https://static-content.springer.com/cover/journal/10606/24/2.jpg" alt="Computer Supported Cooperative Work (CSCW)" itemprop="image"/>
        </a>

</div>
                            </div>
                        </aside>

                        <div class="main-body" data-role="NavigationContainer">
                            <div class="main-body__content">
                                        <div class="cta-button-container u-hide-two-col">
                    <a href="/content/pdf/10.1007%2Fs10606-015-9218-5.pdf" target="_blank" class="c-button c-button--blue c-button__icon-right gtm-pdf-link" title="Download this article in PDF format" rel="noopener noreferrer">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" version="1.1"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill="#0176C3"><g transform="translate(12.000000, 5.000000)"><path d="M7 7.3L7 1C7 0.4 6.6 0 6 0 5.4 0 5 0.4 5 1L5 7.3 3.5 5.7C3.1 5.3 2.5 5.3 2.1 5.7L2.1 5.7C1.7 6.1 1.7 6.7 2.1 7.1L5.3 10.3C5.7 10.7 6.3 10.7 6.7 10.3L9.9 7.1C10.3 6.7 10.3 6.1 9.9 5.7L9.9 5.7C9.5 5.3 8.9 5.3 8.5 5.7L7 7.3 7 7.3ZM0 13C0 12.4 0.5 12 1 12L11 12C11.6 12 12 12.4 12 13 12 13.6 11.5 14 11 14L1 14C0.4 14 0 13.6 0 13L0 13Z"/></g></g></g></svg>
            <span class="hide-text-small">Download</span>
            <span>PDF</span>
        </a>

        </div>



                                <!DOCTYPE html
  SYSTEM "about:legacy-compat">
<div xmlns="http://www.w3.org/1999/xhtml" class="FulltextWrapper"><div class="ArticleHeader main-context"><div id="enumeration" class="enumeration"><p><a href="/journal/10606" title="Computer Supported Cooperative Work (CSCW)"><span class="JournalTitle">Computer Supported Cooperative Work (CSCW)</span></a></p><p class="icon--meta-keyline-before"><span class="ArticleCitation_Year"><time datetime="2015-06">June 2015</time>, </span><span class="ArticleCitation_Volume">Volume 24, </span><a class="ArticleCitation_Issue" href="/journal/10606/24/2/page/1">Issue 2–3</a>,
                       <span class="ArticleCitation_Pages"> pp 79–108</span></p></div><div xmlns="" class="MainTitleSection"><h1 xmlns="http://www.w3.org/1999/xhtml" class="ArticleTitle" lang="en">Towards Concurrent Multi-Tasking in Shareable Interfaces</h1></div><div class="authors u-clearfix" data-component="SpringerLink.Authors"><ul class="u-interface u-inline-list authors__title" data-role="AuthorsNavigation"><li><span>Authors</span></li><li><a href="#authorsandaffiliations" class="gtm-tab-authorsandaffiliations">Authors and affiliations</a></li></ul><div class="authors__list" data-role="AuthorsList"><ul class="test-contributor-names"><li><span itemprop="name" class="authors__name">Carles F. Julià</span><span class="author-information"><span class="authors__contact"><a href="mailto:carles.fernandez@upf.edu" title="carles.fernandez@upf.edu" itemprop="email" class="gtm-email-author">Email author</a></span></span></li><li><span itemprop="name" class="authors__name">Sergi Jordà</span></li></ul></div></div><div class="main-context__container" data-component="SpringerLink.ArticleMetrics"><div class="main-context__column"><span><span class="test-render-category">Article</span></span><div class="article-dates" data-component="SpringerLink.ArticleDates"><dl><dt>First Online: </dt><dd class="article-dates__first-online"><time datetime="2015-03-28">28 March 2015</time></dd></dl><dl class="article-dates__history"></dl></div></div></div></div><section class="Abstract" id="Abs1" tabindex="-1" lang="en"><h2 class="Heading">Abstract</h2><p id="Par1" class="Para">Shareable interfaces, those that can be interacted simultaneously by several users, are a common tool used both in CSCW research and in real world applications. They tend however to lack a capability that has been traditionally relevant to the usefulness of computing systems: multi-tasking. In this paper we explain why a combination of the multi-user features of shareable interfaces and the multi-tasking capabilities of general-purpose computing, could be relevant for building useful systems, and why these features are not present today in most of the current prototypes and systems. We also discuss possible approaches for solving the problems that prevent shareable interfaces to fully support multi-tasking, and we present a novel approach based on a distributed, application-centered, content-based gesture disambiguation. We describe how an already existing framework, GestureAgents, implements this new approach, focusing on expanding the description of the relevant elements related to this problem, and conclude with some example applications and a discussion.</p></section><div class="KeywordGroup" lang="en"><h3 class="Heading">Keywords</h3><span class="Keyword">Concurrent interaction </span><span class="Keyword">Multi-user </span><span class="Keyword">Shareable interfaces </span><span class="Keyword">Multi-tasking </span><span class="Keyword">Agent exclusivity </span></div><div class="note test-pdf-link" id="cobranding-and-download-availability-text"><div>    <a class="gtm-pdf-link" href="/content/pdf/10.1007%2Fs10606-015-9218-5.pdf" target="_blank" rel="noreferrer noopener">Download</a>
 fulltext PDF</div></div><div class="article-actions--inline" id="article-actions--inline" data-component="article-actions--inline"></div><div id="body"><section id="Sec1" tabindex="-1" class="Section1 RenderAsSection1 SectionTypeIntroduction"><h2 class="Heading"><span class="HeadingNumber">1 </span>Introduction</h2><div class="content"><p id="Par2" class="Para">Shareable interfaces are a common subject of study in the field of CSCW. Tabletops and vertical displays, for instance, are considered, in many ways, a good approach to promote collaboration, a circumstance that is valuable for solving complex tasks.</p><p id="Par3" class="Para">In the personal computer context (still the leading professional platform), complex task solving is often supported by the use of a combination of several unrelated software tools. However, the systems developed to study collaboration in shareable interfaces usually feature a single ad-hoc application that tries to cover all the aspects involved in the particular tested task. While this approach is valuable for studying many mechanisms of collaboration, as it constitutes a controlled environment in which the interaction dynamics can be tested, it still does not really represent existing real-world practices.</p><p id="Par4" class="Para">Previous experiences in other kinds of interfaces, such as PCs or hand-held devices, suggest that the real world use of new general purpose computing devices will need some kind of multi-tasking capabilities if they aim to support general and potentially complex task solving features and if, in short, they aspire to become useful to the general public. And yet, multi-tasking features in shareable interfaces may have deep differences even within single-user contexts.</p><p id="Par5" class="Para">This paper first analyzes the diverging relevant strengths of PCs and shared interfaces (Section 2). It then explores how these two sets of strengths are present in current shareable interface devices, and lists relevant work (Section 3). It next analyzes how multi-tasking could be implemented in big shared interfaces; addressing the specific problems this may pose (Section 4). It also presents GestureAgents, a concurrent multi-tasking interaction framework which solves the different issues exposed (Section 5). Several demo applications ant tests are exposed following in Section 6. Discussion is presented in Section 7 and finally conclusions exposed in Section 8.</p></div></section><section id="Sec2" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">2 </span>From personal to collaborative computing</h2><div class="content"><p id="Par6" class="Para">When designing collaborative computing appliances, we are in risk of losing some of the essential elements previously present in personal computing and thus ending up with a useless system. Reviewing the features that made personal computing useful to people can help us to prevent the latter from happening.</p><p id="Par7" class="Para">We here briefly review the qualities that contributed to make the personal computing platform successful and useful to its users. We focus on multi-tasking as a key element to such systems, and an element that we argue, is often overlooked in current collaborative computing systems.</p><p id="Par8" class="Para">We also focus on the key deficiency of personal computers, which collaborative computing intends to solve: collaboration, and more specifically multi-user interaction with computer systems. We think that the combination of these two key elements, collaboration and multi-user interaction, can bring collaborative computing system to a state of usefulness ready to be adopted.</p><section id="Sec3" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">2.1 </span>The rise of the PC</h3><div class="content"><p id="Par9" class="Para">The advent of computers and personal computing devices has had a very deep impact in recent history. Their usefulness relies in their capacity of performing tasks that previously were inconvenient, difficult or impossible, and the core goal of Human-Computer Interaction is indeed devoted to support users accomplishing these tasks (Shaer and Hornecker <span class="CitationRef"><a href="#CR39">2010</a></span>).</p><div id="Par10" class="Para">Many qualities of computers contribute indeed to this goal of easing task solving processes:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p id="Par11" class="Para">Computing power: This was the original function of computers, the ability to compute mathematical operations in an unprecedented speed. There is no need to even try to enumerate the infinity of practical applications that computing power has created, transforming our world.</p></li><li><p id="Par12" class="Para">Convenience: Computers make it easy to perform simple but repetitive tasks, instantly. Boring tasks that may not be especially difficult, when automated, can be performed in a fraction of the time originally needed, thus making them accessible to much more people. Spreadsheets are a good example: accounting existed before computers, but it is now more widely accessible.</p></li><li><p id="Par13" class="Para">Connectivity: It could be argued that portable computing devices do not specially excel in computing power. Still, their usefulness is undeniable, probably because of their connectivity, which allows users to communicate in efficient ways.</p></li><li><p id="Par14" class="Para">General-purpose computing: What makes computers even more useful is that they are generic tools. The purpose and function of a computer can be changed by changing its program. This uncoupling of the device from its function, frees computer builders from having to think about every possible use of the machine. Other parties can create programs that will turn that computer into different specific tools such as a word processor or a calculator. In the smartphone revolution the availability of third-party applications has also arguably been instrumental to their success.</p></li></ul></div></div></div></section><section id="Sec4" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">2.2 </span>Multi-tasking</h3><div class="content"><p id="Par15" class="Para">General-purpose computing allows a single computing device to change its function according to a program. The computer itself has no specific objective as a calculator would have. Logical programs are executed by the computer instead, making it useful for a specific calculation or any task completion. On the other side, as the universe of possible complex tasks and problems to be solved with the assistance of a computer is broad and open, it seems rather unpractical to create a single program for every different task. As complex tasks can often be divided in simpler subtasks, the particularity of every task will often require the use of several different more generic programs, which will address some of the subtasks we can divide the original problem in. Those programs, on its turn, can then be reused for other different tasks.</p><p id="Par16" class="Para">Let’s imagine, for instance, that someone is writing a report on the discoveries of new wildlife in a country. This task will require writing, editing and formatting text, capturing, classifying and editing images, calculating statistics and displaying charts, creating and manipulating maps, etc. Instead of having a single program for “new wildlife finding report writing” involving all these activities, several programs addressing the needs of every single activity can be used: a word processor, an image editor, a file browser, a spreadsheet editor, a map browser, etc. These programs, such as text editors or image viewers, usually designed to solve domain-specific problems, can be also created by parties that do not relate to the creators of the hardware or the programmers of the OS. These third-parties can be programmers or teams that have an expert knowledge of the field the program is focused on. Allowing third party software to be created without the prior consent of the computer, the OS designers or other software creators, allows new programs to continuously appear for filling potential new needs.</p><p id="Par17" class="Para">There is indeed a common agreement that allowing third-party apps is an important factor for success on commercialization of computing platforms. A classic example would be the effect of commercialization of the Lotus 1-2-3 spreadsheet program exclusively for the IBM’s PC. Sales of IBM’s PC had been slow until 1-2-3 was made public, and then increased rapidly a few months after Lotus 1-2-3′s release.<sup><a href="#Fn1" id="Fn1_source">1</a></sup> As a more contemporary example, Apple trademarked the slogan “there is an App for that” for its iPhone 3 g selling campaign on 2009,<sup><a href="#Fn2" id="Fn2_source">2</a></sup> advertising the availability of third-party apps as its main appeal. This move by Apple revolutionized the smartphone scene (West and Mace <span class="CitationRef"><a href="#CR46">2010</a></span>).</p><p id="Par20" class="Para">Modern operating systems and computers allow several programs to be run in parallel, and to switch interaction with the user at any desired time. This ability, multi-tasking, helps to use computers to solve a particular task that involves several steps and requires potentially different programs, in a more convenient way than having to stop the current program to start another. Multi-tasking would be indeed very convenient in our hypothetical new wildlife finding report writing activity: while our user is writing the report, she has the need of inserting a picture of a new specimen. She switches the interaction from the word processor to a file browser to find the picture she wants, she then opens it inside an image editing program (another switch), where she crops the marginal part of the picture. She then switches again to the word processor (that still holds the document she was working on) to insert the modified image.</p><p id="Par21" class="Para">We can thus conclude that multi-tasking is a very desirable feature when creating computing systems, as it provides a way to deal with real world tasks in a convenient fashion, by allowing the use of third party programs.</p></div></section><section id="Sec5" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">2.3 </span>Multi-user</h3><div class="content"><p id="Par22" class="Para">Collaboration has also been traditionally tied to complex task completion: group meetings are a common strategy to shed light into difficult problems. Big problems can be divided into smaller ones that can be redistributed (Strauss <span class="CitationRef"><a href="#CR42">1985</a></span>; Schmidt and Liam <span class="CitationRef"><a href="#CR36">1992</a></span>), and points of view can be exchanged (Hornecker and Buur <span class="CitationRef"><a href="#CR16">2006</a></span>). Even in the computer era, the practice of physical meetings seems to be still (if not more than ever) prevailing. Empowering collaboration with computers is the primary goal of Computer Supported Collaborative Work (CSCW) field, and it relates directly to this group meeting problem. In this discipline, two different (but intersecting) problems are studied: in co-located CSCW all group individuals are present in the same workspace while in remote CSCW individuals are located in different places and all personal interaction is mediated by computers. Both problems deal with several users interacting with the same (local or distributed) system, leading to multi-user interaction.</p><p id="Par23" class="Para">Non co-located settings for multi-user interaction in a single virtual workspace, such as web-based collaborative systems (Bowie et al. <span class="CitationRef"><a href="#CR5">2011</a></span>), or general cases of collective distributed work on single documents (groupware) (Ellis and Gibbs <span class="CitationRef"><a href="#CR10">1989</a></span>), are very common and widely studied. Co-located collaboration around computers, on its turn, already exists in a daily basis. Work meetings are often complemented with laptops, tablets, smartphones and other computing devices.</p><p id="Par24" class="Para">Needless to say, desktop and laptop computers have not been designed for co-located multi-user interaction, but for individual usage. Since they feature a single keyboard and a single pointing device, when used in multi-user setups computers inevitably lead to an interaction “bottleneck” with the users (Stanton et al. <span class="CitationRef"><a href="#CR41">2001</a></span>; Shaer and Hornecker <span class="CitationRef"><a href="#CR39">2010</a></span>). The use of computers in this context is thus still individual, lacking the social affordances that can be provided by “shareable” interfaces, or systems that have specifically been designed for co-located collaboration (Marshall et al. <span class="CitationRef"><a href="#CR33">2007</a></span>). Affordances, which according to scholars such as Hornecker and Buur, should particularly consider Spatial interaction and Embodied Facilitation (Hornecker and Buur <span class="CitationRef"><a href="#CR16">2006</a></span>).</p><p id="Par25" class="Para">Shareable interfaces on their side, alleviate the interaction “bottleneck” by creating multiple interaction points, preventing individuals from taking over control of the computing device (Hornecker and Buur <span class="CitationRef"><a href="#CR16">2006</a></span>). Multiple interaction points do also promote user participation, lowering thresholds for shy people (Hornecker and Buur <span class="CitationRef"><a href="#CR16">2006</a></span>), and can provide means for bi-manual interaction promoting a richer gesture vocabulary (Fitzmaurice et al. <span class="CitationRef"><a href="#CR13">1995</a></span>, <span class="CitationRef"><a href="#CR12">1996</a></span>). A typical type of interfaces developed for these collaborative scenarios are tabletop interfaces, which allow users to interact with horizontal displays using touch and/or pucks; vertical interactive displays (such as interactive whiteboards) in which users interact using pens or touch; tangibles which allow users to interact with physically-embedded artifacts and tokens (Rogers et al. <span class="CitationRef"><a href="#CR48">2009</a></span>; Shaer and Hornecker <span class="CitationRef"><a href="#CR39">2010</a></span>); or body gestural interfaces, such as camera-based systems, which allow users to interact using their bodies (Shaer and Hornecker <span class="CitationRef"><a href="#CR39">2010</a></span>). As a distinct characteristic, all these interfaces allow users a shared access to the same input and output physical interfaces, as opposed to typical groupware systems, where each user has its own interface device (Rogers et al. <span class="CitationRef"><a href="#CR48">2009</a></span>). Besides collaboration, these shareable interfaces show also affordances more directly related with complex task completion. Epistemic actions, physical constraints, and tangible representations of a problem may contribute to problem solving and planning (Shaer and Hornecker <span class="CitationRef"><a href="#CR39">2010</a></span>). Spatial multiplexing allows for a more direct and fast interaction (Fitzmaurice George <span class="CitationRef"><a href="#CR12">1996</a></span>) while leveraging the cognitive load (Shaer and Hornecker <span class="CitationRef"><a href="#CR39">2010</a></span>); tangible objects facilitate creativity (Catalá et al. <span class="CitationRef"><a href="#CR7">2012</a></span>); and rich gestures lighten cognitive load and help in the thinking process while taking advantage of kinesthetic memory (Shaer and Hornecker <span class="CitationRef"><a href="#CR39">2010</a></span>).</p><p id="Par26" class="Para">This combination of social and personal affordances suggest that shareable interfaces are indeed well suited for complex task completion: apart from promoting collaboration, they provide individual and collective benefits that help completing these goals.</p></div></section></div></section><section id="Sec6" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">3 </span>Multi-tasking in shareable interfaces: current situation and related research</h2><div class="content"><p id="Par27" class="Para">Despite all the aforementioned affordances, and considering all the multi-task desirable properties, the majority of the currently available shareable interface systems created for research purposes, consist of a single program that already includes all the necessary facilities to cover every subtask of the main activity. This is, however, consistent with the purpose of most research, because, in a collaboration co-located setting, CSCW researchers typically focus their investigations on the human factors in multi-user interaction, such as how input devices can be more effectively distributed between users in order to optimize group dynamics (Kim and Snow <span class="CitationRef"><a href="#CR28">2013</a></span>; Verma et al. <span class="CitationRef"><a href="#CR44">2013</a></span>), or on studying different strategies to access digital and physical items from the perspective of digital content sharing (Verma et al. <span class="CitationRef"><a href="#CR44">2013</a></span>), control sharing (Jordà et al. <span class="CitationRef"><a href="#CR23">2010</a></span>; Kim and Snow <span class="CitationRef"><a href="#CR28">2013</a></span>), or proxemics (Ballendat et al. <span class="CitationRef"><a href="#CR3">2010</a></span>).</p><p id="Par28" class="Para">A similar enclosing phenomenon happens with real-world products using shareable interfaces. While some of them focus on a very specific domain, avoiding to address more general problems (e.g., the Reactable (Jordà <span class="CitationRef"><a href="#CR21">2008</a></span>) addresses collaboration from the very specific and peculiar needs of musical collaboration (Xambó et al. <span class="CitationRef"><a href="#CR48">2013</a></span>)), many others, such as interactive whiteboards, desist about using any particular multi-user interaction, thus directly presenting the PC graphical system (Beauchamp <span class="CitationRef"><a href="#CR4">2004</a></span>); and when addressing multi-tasking, they are single-tasked, or simply present methods to change the full-screen single active application (Ackad Christopher et al. <span class="CitationRef"><a href="#CR1">2010</a></span>).</p><p id="Par29" class="Para">However, having multi-tasking capabilities in shareable interfaces seems to be in strict consonance with their goal of promoting and enabling collaborative work, as, for instance, the recommendations by Scott et al. for collaborative tabletops (Scott Stacey et al. <span class="CitationRef"><a href="#CR38">2003</a></span>) are related: Multi-tasking provides a way to have simultaneous activities, allowing the transition between them (<em class="EmphasisTypeItalic ">support fluid transitions between activities</em>) and between personal and collective ones (<em class="EmphasisTypeItalic ">support transitions between personal and group work</em>). Also, several tasks can be done concurrently, by several users (<em class="EmphasisTypeItalic ">support simultaneous user actions</em>).</p><p id="Par30" class="Para">It would therefore seem clear that real world shareable interfaces should at least support some of the characteristics that have turned the personal computer into such a valuable tool, such as general purpose computing (and third party application support) and multi-tasking, which sadly are not yet typically found on most current research prototypes.</p><p id="Par31" class="Para">We argue that the lack of those features may not be an accident, neither an unconscious omission: the combination of multi-tasking -a feature so closely associated with single-user devices- with multi-user interaction, is not trivial; even less when combined with rich interfaces such as the ones provided by tabletops. And yet, we want to stress our vision that real world collaborative systems should allow third party applications (programs) to run and be interacted simultaneously. More precisely, every program should support multi-user input, and a single user should be able to interact with several applications at the same time. This is not a novel or revolutionary idea and some works have in fact, previously attempted at the creation of multi-user multi-task systems.</p><p id="Par32" class="Para">Dynamo (Izadi et al. <span class="CitationRef"><a href="#CR20">2003</a></span>), proposes a shared surface for sharing digital data between several (remote) users, focusing on ownership and permissions over programs and documents in a shared multiuser WIMP system. Users may use pairs of mouse-keyboard to interact with a system that presents local and shared interfaces. In shared interfaces it focus the attention on methods for preserving and sharing control over applications and files. It does not, however, deal with co-located access to the interface, nor with third party applications in the shared space.</p><p id="Par33" class="Para">LACOME (Mackenzie et al. <span class="CitationRef"><a href="#CR31">2012</a></span>) also depicts a common shared surface in which remote single-user PC systems are presented as manipulable windows. Third party applications are allowed, but those run in the logic of the former single-user systems. A similar concept is developed in TablePortal (AlAgha et al. <span class="CitationRef"><a href="#CR2">2010</a></span>), where remote tabletop applications and activity is presented inside manipulable windows. In this case remote applications are multi-touch enabled, although its aim is to be used by a single user, the teacher of a classroom.</p><p id="Par34" class="Para">(Ballendat et al. <span class="CitationRef"><a href="#CR3">2010</a></span>) presents us with a series of devices, one of them a vertical shareable interface, which uses information such as the relative positions and orientations of the users, the devices, and other objects, and specifically their pairwise distances (proxemics), for affecting the interaction. As this information is shared between all the devices (as an Ubicomp ecology), and each device can run a different program, we could consider this example as a shared interface (based on the relative positions and orientations) with multi-tasking. The proposal does not describe however any strategy for coordinating the different programs, but rather assumes that they are created together as parts of the same system.</p><p id="Par35" class="Para">WebSurface (Tuddenham et al. <span class="CitationRef"><a href="#CR43">2009</a></span>) presents a tabletop system with virtual windows that can be freely manipulated. These windows are web browsers presenting conventional web pages that can be interacted by the users. It could be argued that web pages are a form of third-party applications, although enclosed in a single-user paradigm. This is also the case of Xplane (Gaggi and Regazzo <span class="CitationRef"><a href="#CR14">2013</a></span>), a software layer presenting several tiled windows on the surface with a distinct focus to enable fast development of tabletop applications, although it does not provide window transformation abilities.</p><p id="Par36" class="Para">Multi Pointer X (MPX) (Hutterer and Thomas Bruce <span class="CitationRef"><a href="#CR18">2007</a></span>) tries to transform PCs into shared systems by allowing them to use several pairs of keyboard and mouse. As PCs are already multi-task and third-party application enabled, the result would be a shared multi-user multi-tasking system. Using a PC setting and applications, however, does not help to easily allow multi-user interaction inside the applications, neither collaboration dynamics related to the physical layout of the interfaces.</p><p id="Par37" class="Para">Julià and Gallardo’s TDesktop (Julià and Gallardo <span class="CitationRef"><a href="#CR24">2007</a></span>) was a first unpublished attempt to create a tabletop operating system. It provided facilities for third-party tabletop applications to be developed, as well as an environment to run and manage multiple applications at the same time. Applications were multi-user by default, and they could ask the system for full-screen execution, when not designed as floating widgets. However, it did not enforce that input events were distributed to one application at most, leaving the possibility to multiple interpretations.</p><p id="Par38" class="Para">In the next section we will study and try to overcome some of the technical and conceptual difficulties for designing a proper multi-tasking system on a shareable interface.</p></div></section><section id="Sec7" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">4 </span>Approaches to multi-tasking</h2><div class="content"><p id="Par39" class="Para">From an implementation point of view, interaction in multi-tasking can be narrowed down to two different problems: (i) allowing two or more processes to share the input and (ii) allowing two or more processes to share the output. In the PC, input would consist of mouse and keyboard events, whereas the output would take place in the monitor display (and in the speakers). In a tabletop system the output would also be the visual and audible display, whereas the input would be provoked by the objects and the finger touches on its surface.</p><p id="Par40" class="Para">Although sharing input and sharing output may be superficially seen as two aspects of the same problem they are fundamentally different. Sharing output is a relatively simple issue because it can be reduced to a mixing mechanism: many programs may require to output some data to a specific destination (the screen), and the task of such a system would simply consist on deciding how to (or rather whether to) mix these data. As the source and destination of the output events is known, the system can use simple rules to decide, for instance, if an app can draw into the display, occluding other programs, or if the sound that it is generating will be mixed with the sounds coming from other programs, and with which volume.</p><p id="Par41" class="Para">On the other hand, sharing input is a much more complicated de-mixing problem: data from one source (such as the data coming from the touch sensor on a multi-touch display) can potentially relate to several recipients, the programs. The task of the system on this case is more complex: the system must know the destination of every data element, that can be shared or not. On a PC, a <em class="EmphasisTypeItalic ">media play</em> keystroke, for instance, has to be distributed to the correct program that is waiting for these types of events, and not always necessarily to the “active” program, the one that is considered to be actually used by the user, with a privileged situation that makes it the default receiver of all input data.</p><section id="Sec8" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">4.1 </span>Input sharing</h3><div class="content"><p id="Par42" class="Para">As many programs can be potential receivers of this data, the system needs a set of rules and mechanisms to fully determine the correct recipient of every piece of input data. These rules will determine the way multi-tasking is presented to the user.</p><p id="Par43" class="Para">Several rules exist, for dealing with this uncertainty. Many of them will take into account the context. In interfaces where input and output happen at the same place (i.e., with Full or Nearby embodiment, according to Fishkin taxonomy (Fishkin <span class="CitationRef"><a href="#CR11">2004</a></span>)), such as in a touchscreen, input events can be tied to the output elements nearby. A touch can be tied to the visual element just underneath it, created by a particular program that will become its correct recipient.</p><p id="Par44" class="Para">Interfaces in which input and output are decoupled (Sharlin et al. <span class="CitationRef"><a href="#CR40">2004</a></span>) may impose more difficulties. When input information is completely untied to the output elements of the processes, strategies other than using a simple distance criterion have to be used. In the case of a mouse device, for instance, the PC strategy is to create a virtual pointer that is controlled by it: as this pointer is coupled to the display it can be treated as in the previous case (coupled). The mouse mediates between the user and the cursor; it is not a generic input device which is part of the interface, but a specific physical representation of the cursor.</p><p id="Par45" class="Para">The PC keyboard is another decoupled interface, and keyboard events can have several destinations, these being different programs or even different widgets inside a program. Some windowing systems simply send the keyboard events to the program under the pointer, while others create a default destination for the keystrokes (Scheifler and Gettys <span class="CitationRef"><a href="#CR35">1990</a></span>). This destination is controlled by the input keyboard focus, so that only one widget (from one application -the active one) is the current receiver of all keyboard activity, and this destination can be changed using the pointer (interacting with another window/widget) or special key combinations (such as Alt-Tab in the PC). The assumption of a single input keyboard focus by the PC interaction makes it difficult to adapt it into a multi-user setting, as the interaction would require multiple foci. Some approaches have been taken in this direction, such as the Multi Pointer X (MPX) (Hutterer and Thomas <span class="CitationRef"><a href="#CR18">2007</a></span>) extension, which allows having virtual input pairs of visual pointers and keyboards that can operate at the same time both with adapted and with legacy X11 applications. It struggles with applications that assume that there is only one pointer and focus, enforcing single-user interaction with those applications as a partial solution. By pairing cursors and keyboards in pairs, MPX allows several foci (one per pair) to simultaneously exist (Hutterer and Thomas <span class="CitationRef"><a href="#CR19">2008</a></span>).</p><p id="Par46" class="Para">The approach to follow on shareable interfaces will depend on the type of interface and its purpose. Coupled input/output interfaces, such as tabletops or vertical displays have the possibility of tying input events to output entities. Gestural body interfaces may have to use other approaches, such as using a mediating virtual representation of the body (equivalent to the cursor) as the seminal work of Myron Krueger in Videoplace or Videodesk (Krueger et al. <span class="CitationRef"><a href="#CR30">1985</a></span>) already suggested, or some other kind of focus mechanism.</p><p id="Par47" class="Para">Input sharing in tabletops is still a young question, as it seems that the problem of multi-tasking has still not arisen. Window-based application management is starting to be present on tables (Tuddenham et al. <span class="CitationRef"><a href="#CR43">2009</a></span>; AlAgha et al. <span class="CitationRef"><a href="#CR2">2010</a></span>) but the preferred option continues to be full screen locking.</p></div></section><section id="Sec9" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">4.2 </span>Area-based interaction</h3><div class="content"><p id="Par48" class="Para">In coupled interfaces it is common to find window-based multi-tasking, so that different programs obtain independent rectangular areas. They can draw and get all the interaction performed inside. Those areas can usually be transformed and manipulated by the user, making it possible for multiple processes to be present in the display at the same time, thus promoting multi-tasking. In these cases, all the programs inputs and outputs are confined inside their respective (or multiple) windows, and a simple coordinate test helps input events to be assigned to the correct program.</p><p id="Par49" class="Para">Rectangular windows particularly fit the PC setting. They have the same shape as the screen, and as they cannot be rotated they can occupy the full screen if necessary, occluding other windows (rotation of windows is not desirable, as the display is vertical and has a well-defined orientation, similarly to what would happen to a painting in a wall).</p><p id="Par50" class="Para">Using windows on other non-PC situations can have some caveats. In non-rectangular interfaces, such as in round tabletops like the Reactable (Jordà <span class="CitationRef"><a href="#CR21">2008</a></span>), the rectangular shape seems to perform poorly. The Reactable’s circular surface was designed to avoid dominant positions (Vernier et al. <span class="CitationRef"><a href="#CR45">2002</a></span>; Jordà et al. <span class="CitationRef"><a href="#CR22">2005</a></span>). While, perhaps for this same reason, the original Reactable avoided the use of windows or rectangular areas, its more recent commercial incarnations make use of them, and allows users to reorient them,<sup><a href="#Fn3" id="Fn3_source">3</a></sup> suggesting that when no predefined orientation exists, the potential rotation of windows seems necessary. Even within rectangular tabletops, at least two (or even four) predominant points of view could exist, making the rotation of windows a desired feature.</p><div id="Par52" class="Para">On top of these orientation issues, forcing a fixed shape for all applications may not always be a convenient solution: some programs may need less restricted areas, leaving most of its window space empty (for instance a circular program such a clock would have considerable empty space at the edges of the window). This empty space would prevent input events to reach other occluded applications, making them unreachable (see Figure <span class="InternalRef"><a href="#Fig1">1</a></span>).<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO1"><a href="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig1_HTML.gif" target="_blank" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig1_HTML.gif" alt=""></img></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Figure 1</span><p class="SimplePara">A window with empty space occluding the interaction for another.</p></div></figcaption></figure></div></div></section><section id="Sec10" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">4.3 </span>Arbitrary shape area-based interaction</h3><div class="content"><p id="Par53" class="Para">An alternative to window-based interaction is area-based interaction. In this case, instead of windows, the system will have to maintain a list of active arbitrary-shape areas of the processes. The input events distribution mechanism should be equivalent as when using windows: a collision test will find the correct program that holds the target area for one particular event. By using arbitrary shapes instead of rectangular windows, processes no longer have the problem of empty occlusion, as all the unused application space does not have to be covered by an area. Using arbitrary-shape areas is already a popular approach when distributing events through different objects inside an application. Inside a program window, the different presented elements define areas where the forwarded input event can be assigned to. Buttons, sliders and many kinds of controls are examples of this strategy.</p><p id="Par54" class="Para">However, this approach is not perfect. Apart from the case of decoupled interfaces, where area-based interaction is not possible, this strategy may not be desirable in other additional situations, at least as the only discriminating mechanism.</p><p id="Par55" class="Para">Recent history of interaction in touch-enabled devices has shown that there is room for improvement beyond the simple gesture primitives that were associated with pointing devices, and a variety of touch-based gestures have been developed and even patented since the first portable multi-touch devices appeared (e.g., pinch zoom, swipe, swipe from outside of the screen, etc.) (Hotelling et al. <span class="CitationRef"><a href="#CR17">2004</a></span>; Elias et al. <span class="CitationRef"><a href="#CR9">2007</a></span>).</p><p id="Par56" class="Para">The fact that portable devices tend to have full-screen applications, which can therefore trivially manage all the multi-touch input, has boosted the development of complementary and often idiosyncratic gestures, able to handle more complex and richer interaction. If areas were used to know the destination of every input event, the gestures of every application should start, continue and end inside of the process’ areas, rendering many gestures that used to temporarily transit outside the target area, impossible to recognize. Even a strategy where only the starting event is used to check the colliding area may have problems with gestures starting outside of it. Let’s imagine and study some examples of gestures that would be problematic when using areas. An application is responsible for displaying notes through the surface of a tabletop. Those notes can be translated and transformed by standard direct manipulation gestures such as pinch zoom or dragging. Imagine that the programmer wants to implement a gesture to save this note: circling the note.</p><div id="Par57" class="Para">Note that for circling a widget with one finger, we do not need to enter in contact with the widget itself (see Figure <span class="InternalRef"><a href="#Fig2">2</a></span>). If the area of the widget is defined by the surface of the note, the needed input events will never reach its right destination. Having a larger gesture area covering the places where gestures are likely to occur may help to receive such events, but at the cost of occluding the interaction with other event recipients, such as other potential applications underneath this note’s area.<figure class="Figure" id="Fig2"><div class="MediaObject" id="MO2"><a href="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig2_HTML.gif" target="_blank" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig2_HTML.gif" alt=""></img></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Figure 2</span><p class="SimplePara">A note-taking application that allows the user to create new notes by drawing circles with a finger over a map browsing application that can be dragged with the finger. Notice that if using area-based interaction, the note taking program will not receive circling events.</p></div></figcaption></figure></div><p id="Par58" class="Para">In this other example, let’s imagine a gesture (e.g., a cross) that instantiates a new widget (e.g., a new note in our note-taking application), anywhere on the interactive surface. As there is no predefined existing area listening for events, the note-taking application cannot know when and where to invoke a new note, and, if the whole-surface area was used for catching all potential crosses, other applications would be occluded and being unable to receive any input event. Although this particular example could be solved by showing a button widget to create new notes, it would have to always be visible, cluttering the space. Global system gestures could be another example of gestures made outside areas, a gesture defined by the system to show a global menu, such as a wave gesture, can be performed anywhere on the surface, regardless of whatever is underneath.</p><p id="Par59" class="Para">Julià and Gallardo’s TDesktop (Julià and Gallardo <span class="CitationRef"><a href="#CR24">2007</a></span>) tabletop operating system solved this problem by allowing the several applications that could run simultaneously to receive the raw stream of input events as an addition from its standard area-based input event filtering, thus receiving also input data that originated elsewhere of their areas. This solution, although effective, rises the problem of how to distribute events through applications, so to avoid the problem of having several subscriber programs receiving the same events, and each of them simultaneously assuming being the intended addressee of the interaction.</p><p id="Par60" class="Para">Finally, the area-based strategy to multi-task interaction is not possible with decoupled interfaces such as full-body sensors and camera-based interfaces (e.g., Kinect), motion sensors (e.g., wiimote), voice and sound interfaces (e.g., Speech recognition), these could still benefit from multi-tasking abilities as they are already used in multi-user contexts. If multi-tasking with decoupled interfaces may still seem like a fringe problem, an example can quickly reveal its need. When multiple home appliances in the same room, such as a hi-fi sound system and an air conditioner, can accept body gestures as commands, they are in fact sharing the same input interface (the body). Some mechanism has to ensure that the same body movement cannot be interpreted as commands for both appliances simultaneously.</p><p id="Par61" class="Para">In short, shareable interfaces (as we have seen in the example of TDesktop) trying to process area-less gestures, but also decoupled interfaces, would benefit from a mechanism different than using areas or windows, for distributing input data to its correct destination, and thus preventing various programs to process the same events.</p></div></section><section id="Sec11" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">4.4 </span>Content/semantics -based input sharing</h3><div class="content"><p id="Par62" class="Para">For decoupled systems that cannot use window (or area)-based input sharing, as well as for coupled interfaces that for some reason would opt for not using it, an alternative can be using content-based input sharing.</p><p id="Par63" class="Para">In a content-based input sharing mechanism, the algorithm, instead of distributing the input events to their destinations based on the position of the event, would try to know which events are expected by every application, and would then distribute these events by deducing their right destination. This approach would not necessarily treat input data as separated events, but rather as streams of events that may convey meaning within them. The destination of an input event, for instance, may not only depend on its own information, but also on the gesture it is part of, on the types and characteristics of the possible recipients, the context, etc. Generally speaking, when a series of input events that have a global meaning/semantics as a gesture is defined, the system’s function is to successfully recognize the performed gesture and subsequently distribute it into the processes, given their current expectations and their contexts. A very simple example implementing this idea could be a system which has the code to recognize a set of gestures from the input, and when it fully recognizes a gesture, this is distributed to the application that has requested it. In the possible case that applications <em class="EmphasisTypeItalic ">A</em> and <em class="EmphasisTypeItalic ">B</em> request respectively the <em class="EmphasisTypeItalic ">stick</em> and <em class="EmphasisTypeItalic ">pinch</em> gestures, when the system recognizes a <em class="EmphasisTypeItalic ">stick</em> gesture it handles it to <em class="EmphasisTypeItalic ">A</em>. Instead, when a <em class="EmphasisTypeItalic ">pinch</em> gesture is recognized this one is sent to <em class="EmphasisTypeItalic ">B</em>.</p><div id="Par64" class="Para">An issue arises when implementing a system that uses content-based input sharing: does the system incorporate all the code needed to recognize all the defined gestures? Should the full set of gestures be defined within the system or should they be defined within the addressees programs themselves? Depending on how we choose to distribute the role of defining and recognizing these gestures, three different strategies can be employed:<div class="OrderedList"><ol><li class="ListItem"><span class="ItemNumber">a.</span><div class="ItemContent"><p id="Par65" class="Para">A centralized gesture recognition engine, with a fixed set of gestures.</p><p id="Par66" class="Para">As in our <em class="EmphasisTypeItalic ">stick</em> and <em class="EmphasisTypeItalic ">pinch gestures</em> example, the system could define a fixed set of gestures the applications could register to. Based on the preferences of the applications at the time a gesture is recognized, the system just notifies the correct program when an individual gesture is recognized. Unfortunately, this strategy has a clear drawback since it prevents programs to define their own gestures, the ones that the application programmer(s) felt were best suited. Rich interaction, understood as the possibility for individual applications to define their own optimal gestures independently of the existing system gestures, is thus dangerously limited.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><span class="ItemNumber">b.</span><div class="ItemContent"><p id="Par67" class="Para">A centralized gesture recognition engine, with an application-defined set of gestures.</p><p id="Par68" class="Para">In this type of systems, common recognizing mechanism needs to be implemented, for which the application programmers will define their own respective recognizable gestures. Many recent advances have been attained in the direction of language-based gesture definitions, especially in the context of multi-touch applications, which in our case could allow arbitrary gesture definitions to be added to the system at runtime:</p><p id="Par69" class="Para">Proton (Kin et al. <span class="CitationRef"><a href="#CR29">2012</a></span>), Midas (Scholliers et al. <span class="CitationRef"><a href="#CR37">2011</a></span>), GeForMT (Kammer et al. <span class="CitationRef"><a href="#CR27">2010b</a></span>) and GISpL (Echtler and Butz <span class="CitationRef"><a href="#CR8">2012</a></span>) all allow the programmer to describe gestures in specially crafted languages that simplify the programming of gesture recognizers, and therefore the code dedicated to detect gestures from the input event streams. From those, Midas, GeForMT and GISpL are interpreted (GISpL only partially) and could theoretically be used as the basis for more general systems, on wich the applications carry their own gesture definitions and transfer such specifications to the system, which would use them to recognize the gestures.</p><p id="Par70" class="Para">The choice of the gesture definition language is also a non-trivial issue. Such a language should ideally be as complete as possible in order not to become an obstacle for the programmers, thus making some gestures impossible to define. For instance, for allowing gestures to be related to the application context data, such as virtual objects inside the application, the definition language should provide ways to access it. Proton, GeForMT and GISpL explicitly integrate areas (as parameters to be accessed in the language or as a previous filtering) as part of their languages, easing area-based gestures to be programmed, but making area-less gestures difficult to describe, as this is a fundamental part of these languages. Midas allows instead for a sort of generic user-defined code and object access from inside the gesture definition, thus enabling not only areas, but also other types of constrains to be used, showing its potential to be useful in many gesture recognition styles. However, it is unclear how such relationship would work when applied on a server–client schema, which would need to interpret the definitions within the system while the needed code and data resides on the program. Apart from these language issues, a gesture recognizer system should also meet some additional requirements. None of the aforementioned languages allow multiple instances of gestures being performed at the same time, treating instead all the input events as part of the same gesture, thus making them unsuitable for multi-user contexts.</p><p id="Par71" class="Para">Although a variation of the previous projects would probably fit the requirements for building this type of system, forcing all the programs to describe their gestures in a common language would also have the side effect of preventing other kinds of gesture-recognition approaches from being used. For instance, machine learning based approaches (such as (Wobbrock et al. <span class="CitationRef"><a href="#CR47">2007</a></span>) or (Caramiaux and Tanaka <span class="CitationRef"><a href="#CR6">2013</a></span>)) would not be possible, since within this strategy, gestures are not formally described, but learned instead from examples.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><span class="ItemNumber">c.</span><div class="ItemContent"><p id="Par72" class="Para">A decentralized application-centered gesture recognition, with a coordination protocol</p><p id="Par73" class="Para">With this third strategy, the system does not participate directly on the recognition of the gestures, but helps instead in coordinating the set of programs interested in these gestures. The recognition process takes therefore place inside the applications, allowing nearly total freedom to the programmer, while a common protocol between the system and the programs is used to guarantee that no single event is mistakenly delivered to two different processes.</p><p id="Par74" class="Para">By running the gesture recognition inside the application, it can take into account its context (e.g., position of the application elements, and other internal logic) without having to rely on a good gesture language definition, as in the previous case. This approach also allows programmers to code the recognizers using their favorite techniques or frameworks, instead of having to rely on the system’s choice of language or libraries. Furthermore, as the system is in charge of preventing double interpretations of gestures across different applications, the different recognizing mechanisms will not need to provide multi-tasking facilities. The aforementioned gesture description languages could be easily adapted to support the coordination protocol with the system, and they could be deployed inside the application. Other programs could for example use a machine learning approach provided that they respect the protocol, and thus train their gesture recognizers with examples.</p><p id="Par75" class="Para">The framework we are presenting, GestureAgents (Julià et al. <span class="CitationRef"><a href="#CR25">2013</a></span>), tries to create this common protocol and infrastructure. In GestureAgents, instead of relying in the use of a particular declarative language, the recognizing mechanism is conditioned by a series of coordination messages that the system and the processes need to exchange.</p></div><div class="ClearBoth"> </div></li></ol></div></div></div></section></div></section><section id="Sec12" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">5 </span>Implementation of GestureAgents framework</h2><div class="content"><p id="Par76" class="Para">In this section, we describe how the GestureAgents framework implements the proposed protocol strategy to manage input events to be consumed by recognizers implemented in several applications. We first introduce the basic elements, then the protocol between the applications and the system, the restrictions of the gesture recognizers’ behavior, and give details about the functioning of the system and the particular implementation of GestureAgents.</p><section id="Sec13" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">5.1 </span>Elements of GestureAgents</h3><div class="content"><div id="Par77" class="Para">GestureAgents is a framework that aims to provide a generic and flexible solution for multi-user interaction in shareable interfaces, both inside a single application as in a multi-tasking system. As schematized in Figure <span class="InternalRef"><a href="#Fig3">3</a></span>, GestureAgents relies upon the concepts of “agent”, “gestures” and “gesture recognizers” and on the idea of “agent exclusivity”.<figure class="Figure" id="Fig3"><div class="MediaObject" id="MO3"><a href="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig3_HTML.gif" target="_blank" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig3_HTML.gif" alt=""></img></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Figure 3</span><p class="SimplePara">Conceptual elements of GestureAgents.</p></div></figcaption></figure></div><p id="Par78" class="Para">An agent is the source component of part of the interface input events, such as an object in contact with a tangible tabletop interface or a finger touching the surface on a touch-based interface. For example, in the case of a multi-touch interface, an agent would be created for every sequence of touches, considering that a sequence starts with the detection of a finger hitting the surface and concludes when the finger is removed from the surface. By default agents will represent the minimal set of identifiable event types (such as the finger touch already described) while more high level agents, those composed by other agents, such as a hand agent composed by finger ones, can be also provided, which are best suited for full body interfaces, where the interaction can have different “resolutions”.</p><p id="Par79" class="Para">Gestures are sequences of agents’ events, which convey meaning expressed by the user and defined by the program. A gesture can relate to a single agent or to multiple ones, both simultaneously and distributed in time. Gestures can be discrete (or symbolic), in which case they will not trigger any reaction until they are finished, or continuous, which already convey meaning before they are completed, and can therefore trigger reactions before finished (Kammer et al. <span class="CitationRef"><a href="#CR26">2010a</a></span>).</p><div id="Par80" class="Para">A gesture recognizer, a piece of code that checks that the pattern that defines the gesture corresponds to the received events, is used by the program to identify a gesture coming from the agents’ events. By using agents as the basis for its gestures, recognizers do not have to receive all the events from the interface, but only the agents they are interested in. This allows the recognition of multiple gestures at the same time (see Figure <span class="InternalRef"><a href="#Fig4">4</a></span>), as opposed to the majority of gesture frameworks, where all the input events are part of the same gesture.<figure class="Figure" id="Fig4"><div class="MediaObject" id="MO4"><a href="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig4_HTML.gif" target="_blank" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig4_HTML.gif" alt=""></img></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Figure 4</span><p class="SimplePara">In this example the events, emitted by two agents (A1,A2) and represented by circles, are part of three different gestures (G1,G2,G3) that can occur simultaneously, as in the case of G2 and G3.</p></div></figcaption></figure></div><div id="Par81" class="Para">The fundamental idea in GestureAgents is based upon <em class="EmphasisTypeItalic ">agent exclusivity</em>. An agent, at one given time, can only be part of one gesture (see Figure <span class="InternalRef"><a href="#Fig5">5</a></span>). The system presents the input data, in the form of agents, to the gesture recognizers inside the applications, and, if they want to use them as a part of their associated gestures, they will have to compete between them to earn the exclusivity over the agent’s use before recognizing their gesture. By locking different agents, several recognizers can simultaneously recognize gestures, preventing double interpretation of the same input events, and allowing multi-tasking and multi-user gesture interaction.<figure class="Figure" id="Fig5"><div class="MediaObject" id="MO5"><a href="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig5_HTML.gif" target="_blank" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig5_HTML.gif" alt=""></img></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Figure 5</span><p class="SimplePara">Agent exclusivity enforces that an agent at a given time can only be part of one single gesture.</p></div></figcaption></figure></div><p id="Par82" class="Para">As GestureAgents does not use a special gestural description language, problems concerning the limits of this framework in terms of completeness or design assumptions do not apply. Definition of gestures is done solely on the applications. Areas, if present, are also implemented at the application level, and tested by the applications’ own gesture recognizers, using their own settings. It is thus up to the programmer to use any existing library to recognize gestures or to code a recognizer from scratch.</p></div></section><section id="Sec14" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">5.2 </span>GestureAgents protocol</h3><div class="content"><p id="Par83" class="Para">The coordination protocol is defined by communication between recognizers (inside applications) and agents (in the system), relating to the process of soliciting agents, getting their exclusivity and releasing them.</p><div id="Par84" class="Para">The communication regarding the recognition of gestures, happens between recognizers (inside the applications) and the system (holding the agents), as shown in Figure <span class="InternalRef"><a href="#Fig6">6</a></span>. The GestureAgents’ protocol defines various types of relationships between recognizers and agents, depending on their internal state. Specifically, a recognizer is considered to follow a process of four distinct steps:<figure class="Figure" id="Fig6"><div class="MediaObject" id="MO6"><a href="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig6_HTML.gif" target="_blank" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig6_HTML.gif" alt=""></img></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Figure 6</span><p class="SimplePara">States of a recognizer.</p></div></figcaption></figure><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p id="Par85" class="Para">Initial state</p><p id="Par86" class="Para">The recognizer is waiting for an agent (of one specific type) to be announced by the system. While in this situation, the recognizer can be considered <em class="EmphasisTypeItalic ">dormant</em> (that it is not related to any active agent or gesture).</p></li><li><p id="Par87" class="Para">Evaluation state</p><p id="Par88" class="Para">The recognizer, which has communicated to the system an interest on one or several agents, is evaluating if their events match a possible gesture, which may or may not be recognized at the end. In this state, the confidence of the recognizer for the hypothesized gesture is not high enough for considering it to be correct or incorrect.</p><p id="Par89" class="Para">Depending on the type of gesture being evaluated, this state can be more or less extended in time. Discrete (or symbolic) gestures will be processed mostly in this state, because their correctness is not fully set until the end of the gesture (Kammer et al. <span class="CitationRef"><a href="#CR27">2010b</a></span>). Continuous gestures, however, can be recognized way before the gesture has ended. In this former case, this state will last as long as the type of the gesture is not confirmed.</p></li><li><p id="Par90" class="Para">Recognition state</p><p id="Par91" class="Para">In this phase the recognizer is confident that the tracked events of the agents match its associated gesture pattern. The transition to this state occurs after two subsequent factors: (i) the recognizer no longer considers the gesture an hypothesis (and so it abandons its evaluation state), and (ii) the system grants the recognizer the exclusivity on the requested agents. In this state the recognizer simply processes the agents’ events to extract control events from the gesture, until the recognizer considers it to have ended.</p></li><li><p id="Par92" class="Para">Failed/finished state</p><p id="Par93" class="Para">In this state the recognizer is no longer active; this can be due to the nonrecognition of the gesture, or to the successful conclusion of the recognized gesture.</p></li></ul></div></div><div id="Par94" class="Para">With this behavior in mind, the protocol is composed of a series of messages that can be exchanged between the recognizer and the system. From the recognizer perspective these would be the messages sendable to the system:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p id="Par95" class="Para">Register (or unregister) to a type of agent</p><p id="Par96" class="Para">If a recognizer is registered to a type of agent (for instance a “touch agent”), when a new agent of this kind appears in the system, the recognizer is notified. This message will typically happen in the recognizer’s initial state.</p></li><li><p id="Par97" class="Para">Register (or unregister) to an agent’s event type</p><p id="Par98" class="Para">Given an agent, the recognizer subscribes to its events. For instance given a touch agent it could be possible to register to its update events (movement, or pressure). In the evaluation state, the recognizer will subscribe or unsubscribe to different type of the agent’s events, depending of the pattern of the associated gesture.</p></li><li><p id="Par99" class="Para">Acquire an Agent (preventing other recognizers of getting its exclusivity)</p><p id="Par100" class="Para">By acquiring an agent, the recognizer expresses its interest on it, communicating the system that it is currently evaluating if the agent is part of a given gesture. This message will be responded by the system with the result of the operation: <em class="EmphasisTypeItalic ">true</em> for success acquiring the agent; <em class="EmphasisTypeItalic ">false</em> for failure acquiring it. This prevents the agent to be assigned to other recognizers (from another program, for instance) until this recognizer dismisses it (due to conclusion or to nonrecognition). The recognizer will typically acquire agents in the evaluation state.</p></li><li><p id="Par101" class="Para">Confirm an Agent (requesting the Agent exclusivity)</p><p id="Par102" class="Para">After successfully acquiring an agent and checking for its events, the recognizer may conclude that it is part of the expected gesture. It then proceeds to confirm it. This message will only be issued by the recognizer when attempting to transition from the evaluation state to the recognition state. The response from the system may not be immediate (we will later address disambiguation delay), and until then the recognizer remains in the evaluation state. If the exclusivity is finally granted by the system, the recognizer will receive a message from the system notifying so. If the system does not grant the exclusivity, it will send a message forcing the recognizer to fail.</p></li><li><p id="Par103" class="Para">Dismissing an Agent</p><p id="Par104" class="Para">An agent can be dismissed in order to be reclaimed by the system, for being assigned to other recognizers. This may happen when a recognizer voluntarily considers that an acquired agent is not part of the expected gesture, or when confirmed agents are part of a gesture that the recognizer considers finalized. Also, when a recognizer fails, all the acquired and confirmed agents are forcefully dismissed.</p></li></ul></div></div><p id="Par105" class="Para">The system will send signals to the recognizer, both (i) in response to its requests, (ii) in the case of acquiring an agent and, on its own prerogative, (iii) for notifying the presence of new agents, (iv) for transmitting agents’ events, (v) for granting the exclusivity over an agent, or (vi) for forcing the recognizer to fail.</p><div id="Par106" class="Para">To illustrate how this protocol works we will detail a possible example of a recognizer’s life-cycle, based on a recognizer that implements the recognition of the gesture “straight line over a widget” in a tabletop system, as represented in Figure <span class="InternalRef"><a href="#Fig7">7</a></span>.<figure class="Figure" id="Fig7"><div class="MediaObject" id="MO7"><a href="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig7_HTML.gif" target="_blank" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig7_HTML.gif" alt=""></img></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Figure 7</span><p class="SimplePara">Protocol representation of the “straight line over a widget” gesture recognition example.</p></div></figcaption></figure></div><p id="Par107" class="Para">In this example, when the application starts, the recognizer is instantiated by the application and it starts in its initial dormant state. It then subscribes to the <em class="EmphasisTypeItalic ">touch agent</em> type to receive new agents’ announcements. Each time the system notifies the recognizer of the presence of a new touch agent, the recognizer checks that this agent is near a widget, as its gesture should be related to one of them. If the touch agent happens to be near a widget, the recognizer declares its interest in the agent by <em class="EmphasisTypeItalic ">acquiring</em> it, and entering into its evaluation state.</p><p id="Par108" class="Para">If this agent is not yet assigned in exclusivity to any other recognizer, the system accepts the query and communicates it to the recognizer, which subscribes to this agent’s movement events, in order to track the trajectory of the touch. While the touch agent slides through the surface, the system sends the corresponding agent events related to this movement. With every update, the recognizer keeps checking if the overall movement is indeed a straight line, and if it is crossing the widget nearby.</p><p id="Par109" class="Para">When the touch crosses the widget, the recognizer notices that the events definitively do match its expected gesture pattern, and it confirms the already acquired touch agent, thus requesting its exclusivity. If at this moment no other recognizer is acquiring it, the system confirms the exclusivity to the recognizer. With this confirmation, the recognizer moves to the recognition state, and starts receiving the events from the touch agent. When the recognizer decides that the gesture is completed, it finishes by dismissing the agent in the process.</p></div></section><section id="Sec15" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">5.3 </span>Restrictions on the behaviors of recognizers</h3><div class="content"><p id="Par110" class="Para">The good functioning of the described protocol depends on the recognizers implementing the protocol correctly, but also on respecting some good practices. In particular, during all the time one recognizer stays in its evaluating state it is preventing other (possibly correct) recognizers to get the agents exclusivity and enter their own recognition states.</p><p id="Par111" class="Para">An ill-coded recognizer, for instance, could just acquire all the agents in the system, and never fail or confirm them. This would indefinitely prevent all other recognizers to successfully earn the agents’ exclusivity and thus no recognizer would ever actually recognize their corresponding gestures.</p><div id="Par112" class="Para">To minimize the disambiguation delay between the recognizer confirming the agents and getting their exclusivity (pictured in Figure <span class="InternalRef"><a href="#Fig8">8</a></span>), recognizers must decide as soon as possible whether a stream of input events can be or not be assigned to a gesture, thus minimizing their stay in the evaluation state.<figure class="Figure" id="Fig8"><div class="MediaObject" id="MO8"><a href="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig8_HTML.gif" target="_blank" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig8_HTML.gif" alt=""></img></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Figure 8</span><p class="SimplePara">Disambiguation delay occurs between the evaluation state (<em class="EmphasisTypeItalic ">black</em>) and the recognition state (<em class="EmphasisTypeItalic ">green</em>).</p></div></figcaption></figure></div><p id="Par113" class="Para">Another consequence of this recognition process protocol is that confirming agents is a final decision. Once a recognizer enters the recognition state, the gesture should always be valid, and if the agent’s events are no longer considered part of the gesture, the recognizer should finish and release all the agents’ exclusivity. The agents can then be used again by other recognizers, in the condition of <em class="EmphasisTypeItalic ">recycled</em> agents, as their appearance is caused by the release form a previous recognizer, instead of being new.</p></div></section><section id="Sec16" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">5.4 </span>The GestureAgents system</h3><div class="content"><p id="Par114" class="Para">The rationale behind these messages is embedded in the functioning of the system while protecting the agent exclusivity. For each agent, the system manages a list of acquiring recognizers (those that are interested in the agent) and a slot for only one completing recognizer (that considers this agent as part of its gesture). When a recognizer acquires the agent, the system simply adds it to this list, unless this agent’s exclusivity is already given.</p><p id="Par115" class="Para">When a recognizer confirms an agent requesting its exclusivity, the system removes the recognizer from the acquired list and puts it into the completing slot. If the slot is not empty, the system decides (via the consultation of several policies) whether or not the new candidate should replace the old one, and the loser (whichever it is) is forced to fail. In general, exclusivity is granted only when the list of acquiring recognizers is empty, which usually happens when alternative acquiring recognizers fail recognizing the gesture and thus dismiss the agent, removing them from the agent’s list of acquiring recognizers.</p><div id="Par116" class="Para">When a recognizer dismisses an agent of which it had its exclusivity, this agent can be used again by other recognizers; the system sets a flag marking it as “recycled” and notifies other interested recognizers as if it was a brand new agent (an overall picture of the states and transitions of an agent is shown in Figure <span class="InternalRef"><a href="#Fig9">9</a></span>).<figure class="Figure" id="Fig9"><div class="MediaObject" id="MO9"><a href="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig9_HTML.gif" target="_blank" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig9_HTML.gif" alt=""></img></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Figure 9</span><p class="SimplePara">Life cycle of an agent.</p></div></figcaption></figure></div><p id="Par117" class="Para">This mechanism actually prevents agents from being used as a part of a gesture, until no other recognizers are interested. When two competing recognizers are sure that an agent is part of their gesture, a decision has to be made. Policies, an ordered list of specific rules that apply to specific situations, will deal with cases of conflict, defining priorities and compatibilities between recognizers.</p><div id="Par118" class="Para">The decisions to be taken by the system can be defined by using two sets of policies, completion_policies and compatibility_policies:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p id="Par119" class="Para">The first are consulted when confirming an agent. They decide whether the new recognizer candidate for exclusivity can replace the old one in the completing slot, defining a priority between two competing recognizers. For instance, a system could decide that recognizers from applications with a given priority, will win over non-prioritized ones, or an application could enforce that a pinch zoom recognizer always wins a drag move recognize</p></li><li><p id="Par120" class="Para">The compatibility_policies are used to decide whether a recognizer can be given the exclusivity over one agent, while another one is still acquiring it. Although at a first glance this may seem as if we were breaking the exclusivity rule, we are in fact only affecting the disambiguation mechanism, as we will still only allow one of the recognizer to use the events from this gesture. What this mechanism is in fact allowing is having recognizers in a “latent” state, which will allow other recognizers to use the agent until they can confirm it, thus finally provoking the recognized gesture to end. Compatibility_policies thus permit defining a priority between a confirmed recognizer and “latent” aspirants.</p></li></ul></div></div><div id="Par121" class="Para">A common generic policy set that could be added to a system using GestureAgents, would be one prioritizing complex gestures over simple gestures. For instance, in a tabletop, prioritizing gestures involving multiple fingers over gestures involving one single finger. If we measure this complexity by the number of acquired agents, it would be simple to define a completion_policy guaranteeing that complex gesture recognizers will be granted the agents’ exclusivity whenever they successfully recognize a gesture, in spite of the less complex gesture recognizers acquiring them:<figure class="Figure" id="Figa"><div class="MediaObject" id="MO10"><a href="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Figa_HTML.gif" target="_blank" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Figa_HTML.gif" alt=""></img></a></div></figure></div><div id="Par122" class="Para">By defining a similar compatibility_policy, we would allow simpler gestures to be recognized until a more complex gesture gets the exclusivity. This pair of polices would also solve the previously mentioned pinch-zoom versus drag-move gesture problem.<figure class="Figure" id="Figb"><div class="MediaObject" id="MO11"><a href="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Figb_HTML.gif" target="_blank" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Figb_HTML.gif" alt=""></img></a></div></figure></div><p id="Par123" class="Para">At this point, it has to be noted that the current implementation is not using yet a real, portable network protocol, but is instead prototyped as a relationship between Python objects inside the system and the application. However it follows this pattern closely. In the current prototype implementation, policies can be defined at many levels, and can be introduced by applications, recognizers or the system itself. In a more conservative implementation, with a network-based coordination protocol, it could be more interesting that system-wide policies would only be defined inside the system, thus preventing arbitrary code from injected application-defined policies to be executed by the system. Application-based policies could be instead enforced at the application level.</p></div></section><section id="Sec17" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">5.5 </span>The GestureAgents recognition framework</h3><div class="content"><p id="Par124" class="Para">Apart from the agent exclusivity coordination protocol for multi-user and multi-touch interaction, GestureAgents provides a gesture recognition framework based on the same agent exclusivity concept. It provides gesture composition (i.e., describing a gesture in terms of a combination of previously defined simpler ones) by stacking layers of agent-recognizer relations, and by considering that recognized gestures can also be agents (such as double-tap agents). The framework also takes advantage of the agent exclusivity competition between recognizers for solving internal disambiguation for simultaneous instance of the same gesture recognizer, by treating them as different gestures that have to compete for the agent’s exclusivity.</p><p id="Par125" class="Para">Recent developments in the framework have simplified the first layer of agent-recognizer relation, the one of the system-recognizer communication. By encapsulating every recognizer relationship tree inside an isolating proxy, the protocol becomes much clearer and eliminates possible incompatibility issues due to the use of the compositing feature of the gesture-recognition framework. In the previous structure, there was no distinction between end-user gestures and sub-gestures.</p></div></section></div></section><section id="Sec18" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">6 </span>Example applications and systems created with GestureAgents</h2><div class="content"><p id="Par126" class="Para">GestureAgents has been used in several systems and applications, testing several aspects of the framework: a concurrency test application, a painting system demo, a map-browsing demo and an orchestra conducting simulator. Unless stated, the examples have been implemented in a Reactable Experience tabletop device.<sup><a href="#Fn4" id="Fn4_source">4</a></sup></p><p id="Par128" class="Para">To test the performance on a multi-user condition, a gesture-performing game has been implemented. The gestures used include Tap, Double Tap, Tap Tempo (4 taps) and a variety of waveforms with different shapes and orientations. Users earn points by performing the correct gesture when asked to. Experiments done in this system show that it is capable of successfully supporting concurrent gesture recognition and interaction (Julià et al. <span class="CitationRef"><a href="#CR25">2013</a></span>).</p><div id="Par129" class="Para">A painting system constituted by two separate applications has also been created to test both the agent exclusivity competition by recognizers, and the effects of the recognition delay (see Figure <span class="InternalRef"><a href="#Fig10">10</a></span>, right). One application has recognizers for the tap, stick (straight line) and paint (free movement) gestures, while another uses a double-tap recognizer in a circular area. The results of the gestures of the first application are reflected in visual elements (lines, dots and traces), while the second application erases the display when a double tap is detected. As the double tap is only valid in a circular area, performing a single tap inside the area would, at first, activate also the double-tap recognizer, to end failing after a timeout call. This setting allowed to observe that the recognition delay introduced by the double-tap happened only inside the area.<figure class="Figure" id="Fig10"><div class="MediaObject" id="MO12"><a href="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig10_HTML.gif" target="_blank" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig10_HTML.gif" alt=""></img></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Figure 10</span><p class="SimplePara">A map browsing application (<em class="EmphasisTypeItalic ">left</em>) and a painting system (<em class="EmphasisTypeItalic ">right</em>) implemented in GestureAgents.</p></div></figcaption></figure></div><p id="Par130" class="Para">A map application, featuring typical pinch zoom and drag move gestures for manipulating a world map, as well as tap and stick gestures for annotating geographical locations and reseting the view respectively, has been created to test the different policies (see Figure <span class="InternalRef"><a href="#Fig10">10</a></span>, left). The relationship between the pinch zoom and the drag move recognizers require the first to be able to overcome the agents completed by the second, thus defining both a compatibility_policy and a completion_policy to achieve the effect.</p><div id="Par131" class="Para">Finally, a fully “decoupled interface” application, consisting of an orchestra conductor simulator for the detection of conductor movements using a depth camera, is currently in development (Gómez et al. <span class="CitationRef"><a href="#CR15">2013</a></span>) (see Figure <span class="InternalRef"><a href="#Fig11">11</a></span>). The use of skeleton-based agents as the basis for the gesture recognition is helping us to clarify how multi-level agents, such as joints, limbs and users, should be used in a decoupled level without affecting the agent exclusivity competition between recognizers.<figure class="Figure" id="Fig11"><div class="MediaObject" id="MO13"><a href="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig11_HTML.gif" target="_blank" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="https://static-content.springer.com/image/art%3A10.1007%2Fs10606-015-9218-5/MediaObjects/10606_2015_9218_Fig11_HTML.gif" alt=""></img></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Figure 11</span><p class="SimplePara">Orchestra conductor gesture recognizer application implemented in GestureAgents.</p></div></figcaption></figure></div><p id="Par132" class="Para">The GestureAgents framework is open source and available to anyone for use and improve. The code can be found in the following repository: <span class="ExternalRef"><a target="_blank" rel="noopener noreferrer" href="https://bitbucket.org/chaosct/gesture-agents"><span class="RefSource">https://bitbucket.org/chaosct/gesture-agents</span></a></span>, and videos of some of the examples can be found at <span class="ExternalRef"><a target="_blank" rel="noopener noreferrer" href="http://carles.fjulia.name/gestureagentsvideos"><span class="RefSource">http://carles.fjulia.name/gestureagentsvideos</span></a></span> .</p></div></section><section id="Sec19" tabindex="-1" class="Section1 RenderAsSection1 SectionTypeDiscussion"><h2 class="Heading"><span class="HeadingNumber">7 </span>Discussion</h2><div class="content"><p id="Par133" class="Para">The GestureAgents approach to provide multi-tasking to shareable interfaces is still in a prototype stage and can primarily serve as a starting point to explore this type of application-centric distributed gesture recognition strategy. This means that many aspects regarding the real world usage of such mechanism are still to be explored and discussed in depth.</p><p id="Par134" class="Para">A typical concern of such system could be its resilience against ill-behaved programs. An application that unintentionally grabs input events without releasing them, could effectively block all other programs from receiving the exclusivity over the agents to fully recognize their gestures, unless specific policies preventing or limiting this type of behavior were implemented.</p><p id="Par135" class="Para">Even malware could register similar or identical gestures to the ones from legit programs in order to <em class="EmphasisTypeItalic ">steal</em> those to insert its malicious content. Again, careful policies would have to be designed to limit this kind of attacks, such as using proximity to areas to prioritize conflicting gestures. However, the experience with PC malware tells us that it is very difficult to be protected from malicious applications.</p><p id="Par136" class="Para">Another security-related concern is whether an application could steal secrets from our interaction with other programs. As in GestureAgents every process can receive all input information to check if it fits a particular gesture, it is sensible to think that key-logger-like applications could be effectively developed. Being the situation similar to the PC’s in this case, we can learn from its implemented strategies to solve that particular problem. Some operating systems implement a way to interact with a specific dialog that is isolated from all the other processes in order to enter a password or to confirm an action that requires specific privileges, a possible solution in GestureAgents could be based on this same idea.</p><p id="Par137" class="Para">Other issues related to efficiency could be relevant. The GestureAgents strategy simply distributes the events to the applications, leaving to them the recognition. In this perspective it does not pose any relevant computing burden. Additionally, the restrictions imposed on the recognizers’ behavior favors incremental gesture recognition approaches, which are computationally cheap. In fact, the informal experience through the different exposed tests and demos, does not clearly reveal any perceptually relevant impact by GestureAgents.</p><p id="Par138" class="Para">That said, in current systems, input events are either processed in a central engine before distributing them to the applications, or are filtered by area (or by focus point) before being processed inside the application. In GestureAgents many applications can be processing the same events at the same time, multiplying the needed processing power. At least with the current implementation, this effect seems inevitable.</p><p id="Par139" class="Para">Existing centralized gesture recognition engines that recognize several hypothetical gestures simultaneously are making efforts to parallelize this processing while guaranteeing soft real-time (Marr et al. <span class="CitationRef"><a href="#CR32">2014</a></span>). In GestureAgents the processing of gestures in different applications would be done in parallel by definition, although without real-time guarantees.</p><p id="Par140" class="Para">Overall, we think that the identification of the problem of the lack and need of multi-user concurrent multitasking, and our approach to the solution contribute to the current state of the art. By proposing a content-based disambiguation instead of an area-based one, GestureAgents approach can be a valid solution for multi-tasking, in both coupled and decoupled shareable interfaces, revealing itself as a generic solution. This can be increasingly relevant for new upcoming decoupled interfaces such as hand tracking sensors or depth cameras, which could benefit from policies and strategies developed for other more popular interfaces.</p></div></section><section id="Sec20" tabindex="-1" class="Section1 RenderAsSection1 SectionTypeConclusion"><h2 class="Heading"><span class="HeadingNumber">8 </span>Conclusions</h2><div class="content"><p id="Par141" class="Para">We have identified and exposed the need of multi-tasking capabilities in shareable multi-user interfaces. We have argued about the utility of multi-tasking when solving complex tasks with computers, and showed that multi-tasking features are currently missing in actual shareable interfaces, despite the fact that one of their main goals is complex task solving through collaboration between users.</p><p id="Par142" class="Para">We argue that this lack is not unintentional but a consequence of the difficulty of adapting current multi-tasking-capable systems into shareable interfaces.</p><p id="Par143" class="Para">An analysis of the complexities of implementing such a system together with a discussion of possible strategies has been carried, revealing that “area-based input events distribution” or “gesture language definition-based” approaches may pose problems in the context of rich interaction and decoupled interfaces. A third approach, using a protocol to control input event distribution but leaving gesture recognition to the application has been described and considered as the best choice.</p><p id="Par144" class="Para">An implementation of this approach, GestureAgents, has been presented as a possible solution, which implements the third of these strategies.</p><p id="Par145" class="Para">Examples of use of the framework have been finally presented, showing some of the possibilities of multi-user multi-tasking interaction and the potential of the framework itself.</p></div></section><div class="FootnoteSection"><div class="Heading">Footnotes</div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn1_source">1</a></span><div class="FootnoteContent" id="Fn1"><p id="Par18" class="Para"><span class="ExternalRef"><a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Killer_application"><span class="RefSource">https://en.wikipedia.org/wiki/Killer_application</span></a></span></p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn2_source">2</a></span><div class="FootnoteContent" id="Fn2"><p id="Par19" class="Para"><span class="ExternalRef"><a target="_blank" rel="noopener noreferrer" href="http://www.trademarkia.com/theres-an-app-for-that-77980556.html"><span class="RefSource">http://www.trademarkia.com/theres-an-app-for-that-77980556.html</span></a></span></p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn3_source">3</a></span><div class="FootnoteContent" id="Fn3"><p id="Par51" class="Para"><span class="ExternalRef"><a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=kYyg-wVYvbo"><span class="RefSource">https://www.youtube.com/watch?v=kYyg-wVYvbo</span></a></span></p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn4_source">4</a></span><div class="FootnoteContent" id="Fn4"><p id="Par127" class="Para"><span class="ExternalRef"><a target="_blank" rel="noopener noreferrer" href="http://www.reactable.com/products/reactable_experience/"><span class="RefSource">http://www.reactable.com/products/reactable_experience/</span></a></span></p></div><div class="ClearBoth"> </div></div></div></div><div id="Acknowledgments" class="Acknowledgments"><h2 class="Heading">Acknowledgments</h2><div class="content"><p class="SimplePara">The research leading to these results has received funding from the European Union Seventh Framework Programme FP7 / 2007–2013 through PHENICX project under grant agreement n° 601166.</p></div></div><aside class="Bibliography" id="Bib1" tabindex="-1"><h3 class="Heading">References</h3><div class="content"><ol class="BibliographyWrapper"><li class="Citation"><div class="CitationContent" id="CR1">Ackad, Christopher James, Anthony Collins, Judy Kay (2010). Switch: exploring the design of application and configuration switching at tabletops. <em class="EmphasisTypeItalic ">ITS’10: ACM Int. Conf. Interact. Tabletops Surfaces. Saarbrücken, Germany</em>. New York: ACM Press, pp. 95–104.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Ackad%2C%20Christopher%20James%2C%20Anthony%20Collins%2C%20Judy%20Kay%20%282010%29.%20Switch%3A%20exploring%20the%20design%20of%20application%20and%20configuration%20switching%20at%20tabletops.%20ITS%E2%80%9910%3A%20ACM%20Int.%20Conf.%20Interact.%20Tabletops%20Surfaces.%20Saarbr%C3%BCcken%2C%20Germany.%20New%20York%3A%20ACM%20Press%2C%20pp.%2095%E2%80%93104."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR2">AlAgha, Iyad, Andrew Hatch, Linxiao Ma, Liz Burd (2010). Towards a teacher-centric approach for multi-touch surfaces in classrooms. <em class="EmphasisTypeItalic ">ITS’10: ACM Int. Conf. Interact. Tabletops Surfaces. Saarbrücken, Germany</em>. New York: ACM Press, pp. 187–196.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=AlAgha%2C%20Iyad%2C%20Andrew%20Hatch%2C%20Linxiao%20Ma%2C%20Liz%20Burd%20%282010%29.%20Towards%20a%20teacher-centric%20approach%20for%20multi-touch%20surfaces%20in%20classrooms.%20ITS%E2%80%9910%3A%20ACM%20Int.%20Conf.%20Interact.%20Tabletops%20Surfaces.%20Saarbr%C3%BCcken%2C%20Germany.%20New%20York%3A%20ACM%20Press%2C%20pp.%20187%E2%80%93196."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR3">Ballendat, Till, Nicolai Marquardt, Saul Greenberg (2010). Proxemic interaction. <em class="EmphasisTypeItalic ">ITS’10: ACM Int. Conf. Interact. Tabletops Surfaces. Saarbrücken, Germany</em>. New York: ACM Press, pp. 121–130.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Ballendat%2C%20Till%2C%20Nicolai%20Marquardt%2C%20Saul%20Greenberg%20%282010%29.%20Proxemic%20interaction.%20ITS%E2%80%9910%3A%20ACM%20Int.%20Conf.%20Interact.%20Tabletops%20Surfaces.%20Saarbr%C3%BCcken%2C%20Germany.%20New%20York%3A%20ACM%20Press%2C%20pp.%20121%E2%80%93130."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR4">Beauchamp, Gary (2004). Teacher use of the interactive whiteboard in primary schools: towards an effective transition framework. <em class="EmphasisTypeItalic ">Technology, Pedagogy and Education</em>, vol. 13, no. 3, pp. 327–348.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener noreferrer" href="http://dx.doi.org/10.1080/14759390400200186"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Teacher%20use%20of%20the%20interactive%20whiteboard%20in%20primary%20schools%3A%20towards%20an%20effective%20transition%20framework&amp;author=G.%20Beauchamp&amp;journal=Technology%2C%20Pedagogy%20and%20Education&amp;volume=13&amp;issue=3&amp;pages=327-348&amp;publication_year=2004"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR5">Bowie, Muriel, Oliver Schmid, Agnes Lisowska Masson, Béat Hirsbrunner (2011). Web-based multipointer interaction on shared displays. <em class="EmphasisTypeItalic ">CSCW’11: Proc. ACM 2011 Conf. Comput. Support. Coop. Work. Hangzhou, China</em>. New York: ACM Press, pp. 609–612.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Bowie%2C%20Muriel%2C%20Oliver%20Schmid%2C%20Agnes%20Lisowska%20Masson%2C%20B%C3%A9at%20Hirsbrunner%20%282011%29.%20Web-based%20multipointer%20interaction%20on%20shared%20displays.%20CSCW%E2%80%9911%3A%20Proc.%20ACM%202011%20Conf.%20Comput.%20Support.%20Coop.%20Work.%20Hangzhou%2C%20China.%20New%20York%3A%20ACM%20Press%2C%20pp.%20609%E2%80%93612."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR6">Caramiaux, Baptiste, Atau Tanaka (2013). Machine Learning of Musical Gestures. <em class="EmphasisTypeItalic ">NIME 2013: Proc. 2013 Conf. New Interfaces Music. Expr</em>. Daejeon &amp; Seoul, pp. 27–30.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Caramiaux%2C%20Baptiste%2C%20Atau%20Tanaka%20%282013%29.%20Machine%20Learning%20of%20Musical%20Gestures.%20NIME%202013%3A%20Proc.%202013%20Conf.%20New%20Interfaces%20Music.%20Expr.%20Daejeon%20%26%20Seoul%2C%20pp.%2027%E2%80%9330."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR7">Catalá, Alejandro, Javier Jaen, Betsy van Dijk, Sergi Jordà (2012). Exploring tabletops as an effective tool to foster creativity traits. <em class="EmphasisTypeItalic ">TEI’12: Proc. Sixth Int. Conf. Tangible, Embed. Embodied Interact. Kingston, Ontario, Canada</em>. New York: ACM Press, pp. 143–150.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Catal%C3%A1%2C%20Alejandro%2C%20Javier%20Jaen%2C%20Betsy%20van%20Dijk%2C%20Sergi%20Jord%C3%A0%20%282012%29.%20Exploring%20tabletops%20as%20an%20effective%20tool%20to%20foster%20creativity%20traits.%20TEI%E2%80%9912%3A%20Proc.%20Sixth%20Int.%20Conf.%20Tangible%2C%20Embed.%20Embodied%20Interact.%20Kingston%2C%20Ontario%2C%20Canada.%20New%20York%3A%20ACM%20Press%2C%20pp.%20143%E2%80%93150."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR8">Echtler, Florian, Andreas Butz (2012). GISpL: Gestures Made Easy. TEI’12: Proc. Sixth Int. Conf. Tangible, Embed. Embodied Interact. Kingston, Ontario, Canada. New York: ACM Press, pp. 233–240.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Echtler%2C%20Florian%2C%20Andreas%20Butz%20%282012%29.%20GISpL%3A%20Gestures%20Made%20Easy.%20TEI%E2%80%9912%3A%20Proc.%20Sixth%20Int.%20Conf.%20Tangible%2C%20Embed.%20Embodied%20Interact.%20Kingston%2C%20Ontario%2C%20Canada.%20New%20York%3A%20ACM%20Press%2C%20pp.%20233%E2%80%93240."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR9">Elias, John Greer, Wayne Carl Westerman, Myra Mary Haggerty (2007). Multi-touch gesture dictionary. US Patent 7,840,912 B2.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Elias%2C%20John%20Greer%2C%20Wayne%20Carl%20Westerman%2C%20Myra%20Mary%20Haggerty%20%282007%29.%20Multi-touch%20gesture%20dictionary.%20US%20Patent%207%2C840%2C912%20B2."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR10">Ellis, Clarence A, Simon J Gibbs (1989). Concurrency control in groupware systems. <em class="EmphasisTypeItalic ">SIGMOD’89: Proc. 1989 ACM SIGMOD Int. Conf. Manag. Data. Seattle, Washington, USA</em>. pp. 399–407.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Ellis%2C%20Clarence%20A%2C%20Simon%20J%20Gibbs%20%281989%29.%20Concurrency%20control%20in%20groupware%20systems.%20SIGMOD%E2%80%9989%3A%20Proc.%201989%20ACM%20SIGMOD%20Int.%20Conf.%20Manag.%20Data.%20Seattle%2C%20Washington%2C%20USA.%20pp.%20399%E2%80%93407."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR11">Fishkin, Kenneth P (2004). A taxonomy for and analysis of tangible interfaces. <em class="EmphasisTypeItalic ">Personal and Ubiquitous Computing</em>, vol. 8, no. 5, pp. 347–358.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener noreferrer" href="http://dx.doi.org/10.1007/s00779-004-0297-4"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=A%20taxonomy%20for%20and%20analysis%20of%20tangible%20interfaces&amp;author=KP.%20Fishkin&amp;journal=Personal%20and%20Ubiquitous%20Computing&amp;volume=8&amp;issue=5&amp;pages=347-358&amp;publication_year=2004"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR12">Fitzmaurice, George W (1996). <em class="EmphasisTypeItalic ">Graspable user interfaces</em>. Ph.D. dissertation. University of Toronto: Graduate Department of Computer Science.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Fitzmaurice%2C%20George%20W%20%281996%29.%20Graspable%20user%20interfaces.%20Ph.D.%20dissertation.%20University%20of%20Toronto%3A%20Graduate%20Department%20of%20Computer%20Science."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR13">Fitzmaurice, George W, Hiroshi Ishii, William AS Buxton (1995). Bricks: laying the foundations for graspable user interfaces. <em class="EmphasisTypeItalic ">CHI’95: Proc. SIGCHI Conf. Hum. Factors Comput. Syst. Denver, Colorado, USA</em>. New York:ACM Press/Addison-Wesley Publishing Co., pp. 442–449.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Fitzmaurice%2C%20George%20W%2C%20Hiroshi%20Ishii%2C%20William%20AS%20Buxton%20%281995%29.%20Bricks%3A%20laying%20the%20foundations%20for%20graspable%20user%20interfaces.%20CHI%E2%80%9995%3A%20Proc.%20SIGCHI%20Conf.%20Hum.%20Factors%20Comput.%20Syst.%20Denver%2C%20Colorado%2C%20USA.%20New%20York%3AACM%20Press%2FAddison-Wesley%20Publishing%20Co.%2C%20pp.%20442%E2%80%93449."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR14">Gaggi, Ombretta, Marco Regazzo (2013). An environment for fast development of tabletop applications. <em class="EmphasisTypeItalic ">ITS’13: Proc. 2013 ACM Int. Conf. Interact. tabletops surfaces. St. Andrews, United Kingdom</em>. New York: ACM Press, pp. 413–416.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Gaggi%2C%20Ombretta%2C%20Marco%20Regazzo%20%282013%29.%20An%20environment%20for%20fast%20development%20of%20tabletop%20applications.%20ITS%E2%80%9913%3A%20Proc.%202013%20ACM%20Int.%20Conf.%20Interact.%20tabletops%20surfaces.%20St.%20Andrews%2C%20United%20Kingdom.%20New%20York%3A%20ACM%20Press%2C%20pp.%20413%E2%80%93416."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR15">Gómez, Emilia, Maarten Grachten, Alan Hanjalic, et al. (2013). PHENICX: Performances as Highly Enriched aNd Interactive Concert Experiences. Open access<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=G%C3%B3mez%2C%20Emilia%2C%20Maarten%20Grachten%2C%20Alan%20Hanjalic%2C%20et%20al.%20%282013%29.%20PHENICX%3A%20Performances%20as%20Highly%20Enriched%20aNd%20Interactive%20Concert%20Experiences.%20Open%20access"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR16">Hornecker, Eva, Jacob Buur (2006). Getting a grip on tangible interaction: a framework on physical space and social interaction. <em class="EmphasisTypeItalic ">Proc. CHI’06: SIGCHI Conf. Hum. Factors Comput. Syst. Montréal, Québec, Canada</em>, New York: ACM Press, pp. 437–446.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Hornecker%2C%20Eva%2C%20Jacob%20Buur%20%282006%29.%20Getting%20a%20grip%20on%20tangible%20interaction%3A%20a%20framework%20on%20physical%20space%20and%20social%20interaction.%20Proc.%20CHI%E2%80%9906%3A%20SIGCHI%20Conf.%20Hum.%20Factors%20Comput.%20Syst.%20Montr%C3%A9al%2C%20Qu%C3%A9bec%2C%20Canada%2C%20New%20York%3A%20ACM%20Press%2C%20pp.%20437%E2%80%93446."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR17">Hotelling, Steve, Joshua A Strickon, Brian Q Huppi, et al. (2004). Gestures for touch sensitive input devices. US Patent 8,479,122 B2.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Hotelling%2C%20Steve%2C%20Joshua%20A%20Strickon%2C%20Brian%20Q%20Huppi%2C%20et%20al.%20%282004%29.%20Gestures%20for%20touch%20sensitive%20input%20devices.%20US%20Patent%208%2C479%2C122%20B2."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR18">Hutterer, Peter, Bruce H Thomas (2007). Groupware support in the windowing system. <em class="EmphasisTypeItalic ">Eighth Australas. User Interface Conf. Ballarat, Australia</em>. Australian Computer Society, Inc., pp. 39–46.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Hutterer%2C%20Peter%2C%20Bruce%20H%20Thomas%20%282007%29.%20Groupware%20support%20in%20the%20windowing%20system.%20Eighth%20Australas.%20User%20Interface%20Conf.%20Ballarat%2C%20Australia.%20Australian%20Computer%20Society%2C%20Inc.%2C%20pp.%2039%E2%80%9346."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR19">Hutterer, Peter, Bruce H Thomas (2008). Enabling co-located ad-hoc collaboration on shared displays. <em class="EmphasisTypeItalic ">Ninth Australas. User Interface Conf. Wollongong, NSW, Australia</em>. Australian Computer Society, Inc., pp. 43–50.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Hutterer%2C%20Peter%2C%20Bruce%20H%20Thomas%20%282008%29.%20Enabling%20co-located%20ad-hoc%20collaboration%20on%20shared%20displays.%20Ninth%20Australas.%20User%20Interface%20Conf.%20Wollongong%2C%20NSW%2C%20Australia.%20Australian%20Computer%20Society%2C%20Inc.%2C%20pp.%2043%E2%80%9350."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR20">Izadi, Shahram, Harry Brignull, Tom Rodden, et al. (2003). Dynamo: a public interactive surface supporting the cooperative sharing and exchange of media. <em class="EmphasisTypeItalic ">UIST’03: Proc. 16th Annu. ACM Symp. User interface Softw. Technol. Vancouver, BC, Canada</em>. pp. 159–168.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Izadi%2C%20Shahram%2C%20Harry%20Brignull%2C%20Tom%20Rodden%2C%20et%20al.%20%282003%29.%20Dynamo%3A%20a%20public%20interactive%20surface%20supporting%20the%20cooperative%20sharing%20and%20exchange%20of%20media.%20UIST%E2%80%9903%3A%20Proc.%2016th%20Annu.%20ACM%20Symp.%20User%20interface%20Softw.%20Technol.%20Vancouver%2C%20BC%2C%20Canada.%20pp.%20159%E2%80%93168."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR21">Jordà, Sergi (2008). On Stage: the Reactable and other Musical Tangibles go Real. <em class="EmphasisTypeItalic ">International Journal of Arts and Technology</em>, vol. 1, no. 3/4, pp. 268–287.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener noreferrer" href="http://dx.doi.org/10.1504/IJART.2008.022363"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=On%20Stage%3A%20the%20Reactable%20and%20other%20Musical%20Tangibles%20go%20Real&amp;author=S.%20Jord%C3%A0&amp;journal=International%20Journal%20of%20Arts%20and%20Technology&amp;volume=1&amp;issue=3%2F4&amp;pages=268-287&amp;publication_year=2008"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR22">Jordà, Sergi, Martin Kaltenbrunner, Günter Geiger, Ross Bencina (2005). The reactable*. <em class="EmphasisTypeItalic ">ICMC 2005: Proc. Int. Comput. Music Conf. Barcelona, Spain</em>. pp. 579–582.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Jord%C3%A0%2C%20Sergi%2C%20Martin%20Kaltenbrunner%2C%20G%C3%BCnter%20Geiger%2C%20Ross%20Bencina%20%282005%29.%20The%20reactable%2A.%20ICMC%202005%3A%20Proc.%20Int.%20Comput.%20Music%20Conf.%20Barcelona%2C%20Spain.%20pp.%20579%E2%80%93582."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR23">Jordà, Sergi, Carles F Julià, Daniel Gallardo (2010). Interactive surfaces and tangibles. <em class="EmphasisTypeItalic ">XRDS: Crossroads, The ACM Magazine for Students</em>, vol. 16, no. 4, pp. 21–28.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Jord%C3%A0%2C%20Sergi%2C%20Carles%20F%20Juli%C3%A0%2C%20Daniel%20Gallardo%20%282010%29.%20Interactive%20surfaces%20and%20tangibles.%20XRDS%3A%20Crossroads%2C%20The%20ACM%20Magazine%20for%20Students%2C%20vol.%2016%2C%20no.%204%2C%20pp.%2021%E2%80%9328."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR24">Julià, Carles F, Daniel Gallardo (2007). <em class="EmphasisTypeItalic ">TDesktop?: Disseny i implementació d’un sistema gràfic tangible</em>. Degree thesis. Universitat Pompeu Fabra.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Juli%C3%A0%2C%20Carles%20F%2C%20Daniel%20Gallardo%20%282007%29.%20TDesktop%3F%3A%20Disseny%20i%20implementaci%C3%B3%20d%E2%80%99un%20sistema%20gr%C3%A0fic%20tangible.%20Degree%20thesis.%20Universitat%20Pompeu%20Fabra."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR25">Julià, Carles F, Nicolas Earnshaw, Sergi Jorda (2013). GestureAgents: an agent-based framework for concurrent multi-task multiuser interaction. <em class="EmphasisTypeItalic ">Proc. 7th Int. Conf. Tangible, Embed. Embodied Interact. Barcelona, Spain</em>. pp. 207–214.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Juli%C3%A0%2C%20Carles%20F%2C%20Nicolas%20Earnshaw%2C%20Sergi%20Jorda%20%282013%29.%20GestureAgents%3A%20an%20agent-based%20framework%20for%20concurrent%20multi-task%20multiuser%20interaction.%20Proc.%207th%20Int.%20Conf.%20Tangible%2C%20Embed.%20Embodied%20Interact.%20Barcelona%2C%20Spain.%20pp.%20207%E2%80%93214."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR26">Kammer, Dietrich, Georg Freitag, Mandy Keck, Markus Wacker (2010a). Taxonomy and Overview of Multi-touch Frameworks: Architecture, Scope and Features. <em class="EmphasisTypeItalic ">Workshop Eng. Patterns Multitouch Interfaces</em>
                <span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Kammer%2C%20Dietrich%2C%20Georg%20Freitag%2C%20Mandy%20Keck%2C%20Markus%20Wacker%20%282010a%29.%20Taxonomy%20and%20Overview%20of%20Multi-touch%20Frameworks%3A%20Architecture%2C%20Scope%20and%20Features.%20Workshop%20Eng.%20Patterns%20Multitouch%20Interfaces%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR27">Kammer, Dietrich, Jan Wojdziak, Mandy Keck, et al. (2010b). Towards a formalization of multi-touch gestures. <em class="EmphasisTypeItalic ">ITS’10: ACM Int. Conf. Interact. Tabletops Surfaces. Saarbrücken, Germany</em>. New York: ACM Press, pp. 49–58.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Kammer%2C%20Dietrich%2C%20Jan%20Wojdziak%2C%20Mandy%20Keck%2C%20et%20al.%20%282010b%29.%20Towards%20a%20formalization%20of%20multi-touch%20gestures.%20ITS%E2%80%9910%3A%20ACM%20Int.%20Conf.%20Interact.%20Tabletops%20Surfaces.%20Saarbr%C3%BCcken%2C%20Germany.%20New%20York%3A%20ACM%20Press%2C%20pp.%2049%E2%80%9358."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR28">Kim, Henna, Sara Snow (2013). Collaboration on a large-scale, multi-touch display: asynchronous interaction and multiple-input use. <em class="EmphasisTypeItalic ">CSCW’13. San Antonio</em>. pp. 165–168.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Kim%2C%20Henna%2C%20Sara%20Snow%20%282013%29.%20Collaboration%20on%20a%20large-scale%2C%20multi-touch%20display%3A%20asynchronous%20interaction%20and%20multiple-input%20use.%20CSCW%E2%80%9913.%20San%20Antonio.%20pp.%20165%E2%80%93168."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR29">Kin, Kenrick, Björn Hartmann, Tony DeRose, Maneesh Agrawala (2012). Proton: Multitouch Gestures as Regular Expressions. <em class="EmphasisTypeItalic ">CHI’12: Proc. SIGCHI Conf. Hum. Factors Comput. Syst. Austin, Texas, USA</em>. New York: ACM Press pp. 2885–2894.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Kin%2C%20Kenrick%2C%20Bj%C3%B6rn%20Hartmann%2C%20Tony%20DeRose%2C%20Maneesh%20Agrawala%20%282012%29.%20Proton%3A%20Multitouch%20Gestures%20as%20Regular%20Expressions.%20CHI%E2%80%9912%3A%20Proc.%20SIGCHI%20Conf.%20Hum.%20Factors%20Comput.%20Syst.%20Austin%2C%20Texas%2C%20USA.%20New%20York%3A%20ACM%20Press%20pp.%202885%E2%80%932894."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR30">Krueger, Myron W, Thomas Gionfriddo, Katrin Hinrichsen (1985). VIDEOPLACE An artificial reality. <em class="EmphasisTypeItalic ">CHI’85</em>. New York: ACM Press pp. 35–40.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Krueger%2C%20Myron%20W%2C%20Thomas%20Gionfriddo%2C%20Katrin%20Hinrichsen%20%281985%29.%20VIDEOPLACE%20An%20artificial%20reality.%20CHI%E2%80%9985.%20New%20York%3A%20ACM%20Press%20pp.%2035%E2%80%9340."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR31">Mackenzie, Russell, Kirstie Hawkey, Kellogg S Booth, et al. (2012). LACOME: a Multi-User Collaboration System for Shared Large Displays. CSCW’12, Washington. New York: ACM Press, pp. 267–268.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Mackenzie%2C%20Russell%2C%20Kirstie%20Hawkey%2C%20Kellogg%20S%20Booth%2C%20et%20al.%20%282012%29.%20LACOME%3A%20a%20Multi-User%20Collaboration%20System%20for%20Shared%20Large%20Displays.%20CSCW%E2%80%9912%2C%20Washington.%20New%20York%3A%20ACM%20Press%2C%20pp.%20267%E2%80%93268."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR32">Marr, Stefan, Thierry Renaux, Lode Hoste, Wolfgang De Meuter (2014). Parallel gesture recognition with soft real-time guarantees. <em class="EmphasisTypeItalic ">Science of Computer Programming</em>, vol. 98, no. 2, pp. 159–183.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Marr%2C%20Stefan%2C%20Thierry%20Renaux%2C%20Lode%20Hoste%2C%20Wolfgang%20De%20Meuter%20%282014%29.%20Parallel%20gesture%20recognition%20with%20soft%20real-time%20guarantees.%20Science%20of%20Computer%20Programming%2C%20vol.%2098%2C%20no.%202%2C%20pp.%20159%E2%80%93183."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR33">Marshall, Paul, Yvonne Rogers, Eva Hornecker (2007). Are Tangible Interfaces Really Any Better Than Other Kinds of Interfaces? <em class="EmphasisTypeItalic ">CHI’07 workshop on Tangible User Interfaces in Context &amp; Theory, 28 April 2007, San Jose, California, USA</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Marshall%2C%20Paul%2C%20Yvonne%20Rogers%2C%20Eva%20Hornecker%20%282007%29.%20Are%20Tangible%20Interfaces%20Really%20Any%20Better%20Than%20Other%20Kinds%20of%20Interfaces%3F%20CHI%E2%80%9907%20workshop%20on%20Tangible%20User%20Interfaces%20in%20Context%20%26%20Theory%2C%2028%20April%202007%2C%20San%20Jose%2C%20California%2C%20USA."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR34">Rogers, Yvonne, Youn-kyung Lim, William Hazlewood, Paul Marshall (2009). Equal Opportunities: Do Shareable Interfaces Promote More Group Participation Than Single User Displays? <em class="EmphasisTypeItalic ">Human-Computer Interaction</em>, vol. 24, no. 1, pp. 79–116.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Rogers%2C%20Yvonne%2C%20Youn-kyung%20Lim%2C%20William%20Hazlewood%2C%20Paul%20Marshall%20%282009%29.%20Equal%20Opportunities%3A%20Do%20Shareable%20Interfaces%20Promote%20More%20Group%20Participation%20Than%20Single%20User%20Displays%3F%20Human-Computer%20Interaction%2C%20vol.%2024%2C%20no.%201%2C%20pp.%2079%E2%80%93116."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR35">Scheifler, Robert W, Jim Gettys (1990). The X window system. <em class="EmphasisTypeItalic ">Software: Practice and Experience</em>, vol. 20, no. S2, pp. S5–S34.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Scheifler%2C%20Robert%20W%2C%20Jim%20Gettys%20%281990%29.%20The%20X%20window%20system.%20Software%3A%20Practice%20and%20Experience%2C%20vol.%2020%2C%20no.%20S2%2C%20pp.%20S5%E2%80%93S34."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR36">Schmidt, Kjeld, Liam Bannon (1992). Taking CSCW seriously. <em class="EmphasisTypeItalic ">Computer Supported Cooperative Work (CSCW)</em>, vol. 1, no. 1–2, pp. 7–40.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Schmidt%2C%20Kjeld%2C%20Liam%20Bannon%20%281992%29.%20Taking%20CSCW%20seriously.%20Computer%20Supported%20Cooperative%20Work%20%28CSCW%29%2C%20vol.%201%2C%20no.%201%E2%80%932%2C%20pp.%207%E2%80%9340."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR37">Scholliers, Christophe, Lode Hoste, Beat Signer, Wolfgang De Meuter (2011). Midas: a declarative multi-touch interaction framework. <em class="EmphasisTypeItalic ">TEI’11: Proc. fifth Int. Conf. Tangible, Embed. embodied Interact. Funchal, Portugal</em>. New York: ACM Press, pp. 49–56.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Scholliers%2C%20Christophe%2C%20Lode%20Hoste%2C%20Beat%20Signer%2C%20Wolfgang%20De%20Meuter%20%282011%29.%20Midas%3A%20a%20declarative%20multi-touch%20interaction%20framework.%20TEI%E2%80%9911%3A%20Proc.%20fifth%20Int.%20Conf.%20Tangible%2C%20Embed.%20embodied%20Interact.%20Funchal%2C%20Portugal.%20New%20York%3A%20ACM%20Press%2C%20pp.%2049%E2%80%9356."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR38">Scott, Stacey D, Karen D Grant, Regan L Mandryk (2003). System guidelines for co-located, collaborative work on a tabletop display. <em class="EmphasisTypeItalic ">ECSCW 2003: Proc. Eighth Eur. Conf. Comput. Support. Coop. Work. Helsinki, Finland</em>. Springer, pp. 159–178.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Scott%2C%20Stacey%20D%2C%20Karen%20D%20Grant%2C%20Regan%20L%20Mandryk%20%282003%29.%20System%20guidelines%20for%20co-located%2C%20collaborative%20work%20on%20a%20tabletop%20display.%20ECSCW%202003%3A%20Proc.%20Eighth%20Eur.%20Conf.%20Comput.%20Support.%20Coop.%20Work.%20Helsinki%2C%20Finland.%20Springer%2C%20pp.%20159%E2%80%93178."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR39">Shaer, Orit, Eva Hornecker (2010). Tangible User Interfaces: Past, Present, and Future Directions. <em class="EmphasisTypeItalic ">Foundations and Trends in Human-Computer Interaction</em>, vol. 3, no. 1–2, pp. 1–137.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Shaer%2C%20Orit%2C%20Eva%20Hornecker%20%282010%29.%20Tangible%20User%20Interfaces%3A%20Past%2C%20Present%2C%20and%20Future%20Directions.%20Foundations%20and%20Trends%20in%20Human-Computer%20Interaction%2C%20vol.%203%2C%20no.%201%E2%80%932%2C%20pp.%201%E2%80%93137."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR40">Sharlin, Ehud, Benjamin Watson, Yoshifumi Kitamura, et al. (2004). On tangible user interfaces, humans and spatiality. <em class="EmphasisTypeItalic ">Personal and Ubiquitous Computing</em>, vol. 8, no. 5, pp. 338–346.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Sharlin%2C%20Ehud%2C%20Benjamin%20Watson%2C%20Yoshifumi%20Kitamura%2C%20et%20al.%20%282004%29.%20On%20tangible%20user%20interfaces%2C%20humans%20and%20spatiality.%20Personal%20and%20Ubiquitous%20Computing%2C%20vol.%208%2C%20no.%205%2C%20pp.%20338%E2%80%93346."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR41">Stanton, Danae, Tony Pridmore, Victor Bayon, et al. (2001). Classroom collaboration in the design of tangible interfaces for storytelling. <em class="EmphasisTypeItalic ">CHI’01: Proc. SIGCHI Conf. Hum. factors Comput. Syst. Seattle, Washington, USA</em>. New York: ACM Press, pp. 482–489.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Stanton%2C%20Danae%2C%20Tony%20Pridmore%2C%20Victor%20Bayon%2C%20et%20al.%20%282001%29.%20Classroom%20collaboration%20in%20the%20design%20of%20tangible%20interfaces%20for%20storytelling.%20CHI%E2%80%9901%3A%20Proc.%20SIGCHI%20Conf.%20Hum.%20factors%20Comput.%20Syst.%20Seattle%2C%20Washington%2C%20USA.%20New%20York%3A%20ACM%20Press%2C%20pp.%20482%E2%80%93489."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR42">Strauss, Anselm (1985). Work and the Division of Labor. <em class="EmphasisTypeItalic ">The Sociological Quarterly</em>, vol. 26, no. 1, pp. 1–19.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener noreferrer" href="http://dx.doi.org/10.1111/j.1533-8525.1985.tb00212.x"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Work%20and%20the%20Division%20of%20Labor&amp;author=A.%20Strauss&amp;journal=The%20Sociological%20Quarterly&amp;volume=26&amp;issue=1&amp;pages=1-19&amp;publication_year=1985"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR43">Tuddenham, Philip, Ian Davies, Peter Robinson (2009). WebSurface. <em class="EmphasisTypeItalic ">ITS’09: Proc. ACM Int. Conf. Interact. Tabletops Surfaces. Banff, Alberta, Canada</em>. New York: ACM Press, pp. 181–188.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Tuddenham%2C%20Philip%2C%20Ian%20Davies%2C%20Peter%20Robinson%20%282009%29.%20WebSurface.%20ITS%E2%80%9909%3A%20Proc.%20ACM%20Int.%20Conf.%20Interact.%20Tabletops%20Surfaces.%20Banff%2C%20Alberta%2C%20Canada.%20New%20York%3A%20ACM%20Press%2C%20pp.%20181%E2%80%93188."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR44">Verma, Himanshu, Flaviu Roman, Silvia Magrelli, et al. (2013). Complementarity of input devices to achieve knowledge sharing in meetings. <em class="EmphasisTypeItalic ">CSCW’13: Proc. 2013 Conf. Comput. Support. Coop. Work. San Antonio, Texas, USA</em>. ACM, pp. 701–703.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Verma%2C%20Himanshu%2C%20Flaviu%20Roman%2C%20Silvia%20Magrelli%2C%20et%20al.%20%282013%29.%20Complementarity%20of%20input%20devices%20to%20achieve%20knowledge%20sharing%20in%20meetings.%20CSCW%E2%80%9913%3A%20Proc.%202013%20Conf.%20Comput.%20Support.%20Coop.%20Work.%20San%20Antonio%2C%20Texas%2C%20USA.%20ACM%2C%20pp.%20701%E2%80%93703."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR45">Vernier, Frédéric, Neal Lesh, Chia Shen (2002). Visualization techniques for circular tabletop interfaces. <em class="EmphasisTypeItalic ">AVI’02: Proc. Work. Conf. Adv. Vis. Interfaces Trento, Italy</em>. New York: ACM Press, pp. 257–266.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Vernier%2C%20Fr%C3%A9d%C3%A9ric%2C%20Neal%20Lesh%2C%20Chia%20Shen%20%282002%29.%20Visualization%20techniques%20for%20circular%20tabletop%20interfaces.%20AVI%E2%80%9902%3A%20Proc.%20Work.%20Conf.%20Adv.%20Vis.%20Interfaces%20Trento%2C%20Italy.%20New%20York%3A%20ACM%20Press%2C%20pp.%20257%E2%80%93266."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR46">West, Joel, Michael Mace (2010). Browsing as the killer app: Explaining the rapid success of Apple’s iPhone. <em class="EmphasisTypeItalic ">Telecommunications Policy</em>, vol. 34, no. 5–6, pp. 270–286.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=West%2C%20Joel%2C%20Michael%20Mace%20%282010%29.%20Browsing%20as%20the%20killer%20app%3A%20Explaining%20the%20rapid%20success%20of%20Apple%E2%80%99s%20iPhone.%20Telecommunications%20Policy%2C%20vol.%2034%2C%20no.%205%E2%80%936%2C%20pp.%20270%E2%80%93286."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR47">Wobbrock, Jacob O, Andrew D Wilson, Yang Li (2007). Gestures without libraries, toolkits or training: a $1 recognizer for user interface prototypes. <em class="EmphasisTypeItalic ">UIST’07: Proc. 20th Annu. ACM Symp. User interface Softw. Technol. Newport, Rhode Island, USA</em>. New York: ACM Press, pp. 159–168.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Wobbrock%2C%20Jacob%20O%2C%20Andrew%20D%20Wilson%2C%20Yang%20Li%20%282007%29.%20Gestures%20without%20libraries%2C%20toolkits%20or%20training%3A%20a%20%241%20recognizer%20for%20user%20interface%20prototypes.%20UIST%E2%80%9907%3A%20Proc.%2020th%20Annu.%20ACM%20Symp.%20User%20interface%20Softw.%20Technol.%20Newport%2C%20Rhode%20Island%2C%20USA.%20New%20York%3A%20ACM%20Press%2C%20pp.%20159%E2%80%93168."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR48">Xambó, Anna, Eva Hornecker, Paul Marshall, et al. (2013). Let’s jam the reactable. <em class="EmphasisTypeItalic ">ACM Transactions on Computer-Human Interaction</em>, vol. 20, no. 6, pp. 1–34.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Xamb%C3%B3%2C%20Anna%2C%20Eva%20Hornecker%2C%20Paul%20Marshall%2C%20et%20al.%20%282013%29.%20Let%E2%80%99s%20jam%20the%20reactable.%20ACM%20Transactions%20on%20Computer-Human%20Interaction%2C%20vol.%2020%2C%20no.%206%2C%20pp.%201%E2%80%9334."><span><span>Google Scholar</span></span></a></span></span></div></li></ol></div></aside><section class="Section1 RenderAsSection1"><h2 class="Heading" id="copyrightInformation">Copyright information</h2><div class="ArticleCopyright content"><div class="ArticleCopyright">© Springer Science+Business Media Dordrecht 2015</div></div></section><section id="authorsandaffiliations" class="Section1 RenderAsSection1"><h2 class="Heading">Authors and Affiliations</h2><div class="content authors-affiliations u-interface"><ul class="test-contributor-names"><li><span itemprop="name" class="authors-affiliations__name">Carles F. Julià</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul><span class="author-information"><span class="author-information__contact u-icon-before icon--email-before"><a href="mailto:carles.fernandez@upf.edu" title="carles.fernandez@upf.edu" itemprop="email" class="gtm-email-author">Email author</a></span></span></li><li><span itemprop="name" class="authors-affiliations__name">Sergi Jordà</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul></li></ul><ol class="test-affiliations"><li class="affiliation" data-test="affiliation-1" data-affiliation-highlight="affiliation-1" itemprop="affiliation" itemscope="" itemtype="http://schema.org/Organization"><span class="affiliation__count">1.</span><span class="affiliation__item"><span itemprop="name" class="affiliation__name">Universitat Pompeu Fabra</span><span itemprop="city" class="affiliation__city">Barcelona</span><span itemprop="country" class="affiliation__country">Spain</span></span></li></ol></div></section></div>
                                <aside class="content-type-about" id="AboutThisContent">
    <h2 class="Heading" id="aboutcontent">About this article</h2>
    <div class="content bibliographic-information">
                <div id="crossMark" class="crossmark">
            <a data-crossmark="10.1007%2Fs10606-015-9218-5" class="gtm-crossmark" target="_blank" rel="noopener noreferrer" href="https://crossmark.crossref.org/dialog/?doi=10.1007%2Fs10606-015-9218-5" title="Verify currency and authenticity via CrossMark">
                <span class="u-screenreader-only">CrossMark</span>
                <svg class="CrossMark" id="crossmark-icon" width="57" height="81">
                    <image width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="/springerlink-static/373808856/images/png/crossmark.png" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/springerlink-static/373808856/images/svg/crossmark.svg"></image>
                </svg>
            </a>
        </div>

            <div class="crossmark__adjacent">
                <div class="u-mb-16">
	<dl class="article-cite">
		<dt class="test-cite-heading">
			Cite this article as:
		</dt>
		<dd id="citethis-text">Julià, C.F. & Jordà, S. Comput Supported Coop Work (2015) 24: 79. https://doi.org/10.1007/s10606-015-9218-5</dd>
	</dl>
</div>

                    <ul class="bibliographic-information__list bibliographic-information__list--inline">
        <li class="bibliographic-information__item">
            <span class="bibliographic-information__title"><abbr title="Digital Object Identifier">DOI</abbr></span>
            <span class="bibliographic-information__value" id="doi-url">https://doi.org/10.1007/s10606-015-9218-5</span>
        </li>
            <li class="bibliographic-information__item">
                <span class="bibliographic-information__title">Publisher Name</span>
                <span class="bibliographic-information__value" id="publisher-name">Springer Netherlands</span>
            </li>
            <li class="bibliographic-information__item">
                <span class="bibliographic-information__title">Print ISSN</span>
                <span class="bibliographic-information__value" id="print-issn">0925-9724</span>
            </li>
            <li class="bibliographic-information__item ">
                <span class="bibliographic-information__title">Online ISSN</span>
                <span class="bibliographic-information__value" id="electronic-issn">1573-7551</span>
            </li>

        
    </ul>

                <ul class="bibliographic-information__list">
    <li class="bibliographic-information__item">
        <a id="about-journal" class="bibliographic-information__misc-links gtm-about-this" title="Visit Springer.com for information about this article&#39;s journal"
           href="//www.springer.com/journal/10606/about">About this journal</a>
    </li>
    <li class="bibliographic-information__item">
        <a id="reprintsandpermissions-link" class="u-external" target="_blank" rel="noopener noreferrer" href="https://s100.copyright.com/AppDispatchServlet?publisherName&#x3D;Springer&amp;orderBeanReset&#x3D;true&amp;orderSource&#x3D;SpringerLink&amp;author&#x3D;Carles+F.+Juli%C3%A0&amp;authorEmail&#x3D;carles.fernandez%40upf.edu&amp;issueNum&#x3D;2&amp;contentID&#x3D;10.1007%2Fs10606-015-9218-5&amp;openAccess&#x3D;false&amp;endPage&#x3D;108&amp;publicationDate&#x3D;2015&amp;startPage&#x3D;79&amp;volumeNum&#x3D;24&amp;title&#x3D;Towards+Concurrent+Multi-Tasking+in+Shareable+Interfaces&amp;imprint&#x3D;Springer+Science%2BBusiness+Media+Dordrecht&amp;publication&#x3D;0925-9724&amp;authorAddress&#x3D;Roc+Boronat+138%2C+08018%2C+Barcelona%2C+Spain" title="Visit RightsLink for information about reusing this article">Reprints and Permissions</a>
    </li>
</ul>
            </div>
        
            
    </div>
</aside>

                                <div class="collapsible-section uptodate-recommendations gtm-recommendations">
    <h2 class="uptodate-recommendations__title collapsible-section__heading gtm-recommendations__title" id="uptodaterecommendations">Personalised recommendations</h2>
    <div class="collapsible-section__content">
        <div class="uptodate-recommendations__container">
             <link rel="uptodate-inline" href="/springerlink-static/373808856/css/recommendations.css"/>
        </div>
    </div>
</div>
                                            <div class="sticky-banner u-interface u-js-screenreader-only" aria-hidden="true" data-component="SpringerLink.StickyBanner" data-namespace="hasButton">
                <div class="sticky-banner__container">
                        <div class="citations" data-component="SV.Dropdown" data-namespace="citationsSticky">
        <h3 class="u-h4" data-role="button-dropdown__title">
    <span>Export</span>
    <span class="hide-text-small">citation</span>
</h3>
<ul class="citations__content" data-role="button-dropdown__content">
        <li>
            <a href="//citation-needed.springer.com/v2/references/10.1007/s10606-015-9218-5?format&#x3D;refman&amp;flavour&#x3D;citation"
               title="Download this article&#39;s citation as a .RIS file" class="gtm-export-citation" data-gtmlabel="RIS">
                <span class="citations__extension" data-gtmlabel="RIS">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"/></svg>
                    .RIS
                </span>
                <span class="citations__types">
                        <span>
                            Papers
                        </span>
                        <span>
                            Reference Manager
                        </span>
                        <span>
                            RefWorks
                        </span>
                        <span>
                            Zotero
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="//citation-needed.springer.com/v2/references/10.1007/s10606-015-9218-5?format&#x3D;endnote&amp;flavour&#x3D;citation"
               title="Download this article&#39;s citation as a .ENW file" class="gtm-export-citation" data-gtmlabel="ENW">
                <span class="citations__extension" data-gtmlabel="ENW">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"/></svg>
                    .ENW
                </span>
                <span class="citations__types">
                        <span>
                            EndNote
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="//citation-needed.springer.com/v2/references/10.1007/s10606-015-9218-5?format&#x3D;bibtex&amp;flavour&#x3D;citation"
               title="Download this article&#39;s citation as a .BIB file" class="gtm-export-citation" data-gtmlabel="BIB">
                <span class="citations__extension" data-gtmlabel="BIB">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"/></svg>
                    .BIB
                </span>
                <span class="citations__types">
                        <span>
                            BibTeX
                        </span>
                        <span>
                            JabRef
                        </span>
                        <span>
                            Mendeley
                        </span>
                </span>
            </a>
        </li>
</ul>
    </div>

                        <div class="share-this" data-component="SV.Dropdown" data-namespace="shareThisSticky">
        <h3 class="u-h4" data-role="button-dropdown__title">
    <span>Share</span>
    <span class="hide-text-small">article</span>
</h3>
<ul class="share-this__content" data-role="button-dropdown__content">
    <li>
        <a class="test-shareby-email-link gtm-shareby-email-link" href="mailto:?to&#x3D;&amp;subject&#x3D;Read%20this%20article%20on%20SpringerLink&amp;body&#x3D;Towards%20Concurrent%20Multi-Tasking%20in%20Shareable%20Interfaces%0A%0Ahttps%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10606-015-9218-5" title="Share this article via email">
            <span class="share-this__types">
                <svg width="24" height="24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none"><path fill-opacity="0" fill="#fff" d="M0 0h24v24h-24z"/><g transform="translate(3 5)"><rect fill="#F9F9F9" width="18" height="14" rx="2"/><path d="M1 2.006v9.988c0 .557.446 1.006.995 1.006h14.01c.549 0 .995-.449.995-1.006v-9.988c0-.557-.446-1.006-.995-1.006h-14.01c-.549 0-.995.449-.995 1.006zm-1 0c0-1.108.893-2.006 1.995-2.006h14.01c1.102 0 1.995.897 1.995 2.006v9.988c0 1.108-.893 2.006-1.995 2.006h-14.01c-1.102 0-1.995-.897-1.995-2.006v-9.988zM9 9l7-4v-1.443l-7 4-7-4v1.443z" fill="#666"/></g></g></svg>
                <span>Email</span>
            </span>
        </a>
    </li>
    <li>
        <a class="test-shareby-facebook-link gtm-shareby-facebook-link" href="https://www.facebook.com/sharer/sharer.php?u&#x3D;https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10606-015-9218-5" target="_blank" rel="noopener noreferrer" title="Share this article via Facebook">
            <span class="share-this__types">
                <svg width="24" height="24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none"><path fill-opacity="0" fill="#fff" d="M0 0h24v24h-24z"/><path d="M12.717 19.091h-2.66v-6.274h-1.329v-2.162h1.329v-1.298c0-1.763.75-2.813 2.883-2.813h1.775v2.162h-1.11c-.83 0-.885.302-.885.866l-.004 1.082h2.011l-.235 2.162h-1.775v6.274z" fill="#666"/></g></svg>
                <span>Facebook</span>
            </span>
        </a>
    </li>
    <li>
        <a class="test-shareby-twitter-link gtm-shareby-twitter-link" href="https://twitter.com/intent/tweet?text&#x3D;I%27m%20reading%20this%20on%20%23springerlink&amp;url&#x3D;https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10606-015-9218-5" target="_blank" rel="noopener noreferrer" title="Share this article via Twitter">
            <span class="share-this__types">
                <svg width="24" height="24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none"><path fill-opacity="0" fill="#fff" d="M0 0h24v24h-24z"/><path d="M16.651 9.189c.485-.306.858-.792 1.033-1.371-.454.284-.957.49-1.493.601-.428-.482-1.039-.783-1.715-.783-1.298 0-2.349 1.11-2.349 2.478 0 .194.019.384.06.564-1.952-.104-3.684-1.089-4.843-2.59-.202.367-.318.793-.318 1.247 0 .859.415 1.618 1.045 2.063-.385-.013-.748-.126-1.065-.31v.03c0 1.201.809 2.203 1.886 2.43-.198.058-.405.087-.62.087-.151 0-.299-.015-.442-.044.299.984 1.166 1.702 2.195 1.721-.805.665-1.818 1.061-2.919 1.061-.19 0-.377-.011-.561-.034 1.04.703 2.275 1.113 3.602 1.113 4.323 0 6.686-3.777 6.686-7.052l-.006-.321c.459-.35.859-.786 1.173-1.283-.422.197-.875.33-1.349.39z" fill="#666"/></g></svg>
                <span>Twitter</span>
            </span>
        </a>
    </li>
        <li>
            <a class="test-shareby-linkedin-link gtm-shareby-linkedin-link" href="https://www.linkedin.com/shareArticle?mini&#x3D;true&amp;url&#x3D;https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10606-015-9218-5&amp;title&#x3D;Towards Concurrent Multi-Tasking in Shareable Interfaces&amp;summary&#x3D;Shareable interfaces, those that can be interacted simultaneously by several users, are a common tool used both in CSCW research and in real world applications. They tend however to lack a capability that has been traditionally relevant to the usefu…" target="_blank" rel="noopener noreferrer" title="Share this article via LinkedIn">
                <span class="share-this__types">
                    <svg width="24" height="24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none"><path fill-opacity="0" fill="#fff" d="M0 0h24v24h-24z"/><path d="M6.821 10.044h2.126v7.41h-2.126v-7.41zm1.009-.927h-.015c-.77 0-1.269-.566-1.269-1.284 0-.732.514-1.287 1.299-1.287.784 0 1.267.554 1.282 1.285 0 .717-.498 1.286-1.297 1.286zm9.625 8.338h-2.411v-3.835c0-1.004-.377-1.688-1.206-1.688-.634 0-.987.462-1.151.908-.062.159-.052.382-.052.606v4.01h-2.389s.031-6.794 0-7.411h2.389v1.163c.141-.509.904-1.234 2.122-1.234 1.511 0 2.698 1.067 2.698 3.361v4.121z" fill="#666"/></g></svg>
                    <span>LinkedIn</span>
                </span>
            </a>
        </li>
        <li>
            <a class="gtm-shareby-sharelink-link" data-test="shareable-link" href="/sharelink/10.1007/s10606-015-9218-5" target="_blank" rel="noopener noreferrer" title="Get shareable link">
                <span class="share-this__types">
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="#666" d="M9 7c-2.8 0-5 2.2-5 5s2.2 5 5 5h2v-2h-2c-1.7 0-3-1.3-3-3s1.3-3 3-3h2v-2h-2zm6 0c2.8 0 5 2.2 5 5s-2.2 5-5 5h-2v-2h2c1.7 0 3-1.3 3-3s-1.3-3-3-3h-2v-2h2zm-1 4c.5 0 1 .4 1 1s-.5 1-1 1h-4c-.5 0-1-.4-1-1s.5-1 1-1h4z"/></svg>
                    <span>Shareable link</span>
                </span>
            </a>
        </li>
</ul>
    </div>

                                    <a href="/content/pdf/10.1007%2Fs10606-015-9218-5.pdf" target="_blank" class="c-button c-button--blue c-button__icon-right gtm-pdf-link" title="Download this article in PDF format" rel="noopener noreferrer">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" version="1.1"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill="#0176C3"><g transform="translate(12.000000, 5.000000)"><path d="M7 7.3L7 1C7 0.4 6.6 0 6 0 5.4 0 5 0.4 5 1L5 7.3 3.5 5.7C3.1 5.3 2.5 5.3 2.1 5.7L2.1 5.7C1.7 6.1 1.7 6.7 2.1 7.1L5.3 10.3C5.7 10.7 6.3 10.7 6.7 10.3L9.9 7.1C10.3 6.7 10.3 6.1 9.9 5.7L9.9 5.7C9.5 5.3 8.9 5.3 8.5 5.7L7 7.3 7 7.3ZM0 13C0 12.4 0.5 12 1 12L11 12C11.6 12 12 12.4 12 13 12 13.6 11.5 14 11 14L1 14C0.4 14 0 13.6 0 13L0 13Z"/></g></g></g></svg>
            <span class="hide-text-small">Download</span>
            <span>PDF</span>
        </a>

                </div>
            </div>




                            </div>
                        </div>

                        <aside class="main-sidebar-right u-interface">
                            <div data-role="sticky-wrapper">
                                <div class="main-sidebar-right__content u-composite-layer" data-component="SpringerLink.StickySidebar">
                                    <div class="article-actions" id="article-actions">
                                        <h2 class="u-screenreader-only">Actions</h2>


                                        <div class="u-js-hide u-js-show-two-col">
                                            

                                                    <div class="download-article test-pdf-link">
                                                                <a href="/content/pdf/10.1007%2Fs10606-015-9218-5.pdf" target="_blank" class="c-button c-button--blue c-button__icon-right gtm-pdf-link" title="Download this article in PDF format" rel="noopener noreferrer">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" version="1.1"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill="#0176C3"><g transform="translate(12.000000, 5.000000)"><path d="M7 7.3L7 1C7 0.4 6.6 0 6 0 5.4 0 5 0.4 5 1L5 7.3 3.5 5.7C3.1 5.3 2.5 5.3 2.1 5.7L2.1 5.7C1.7 6.1 1.7 6.7 2.1 7.1L5.3 10.3C5.7 10.7 6.3 10.7 6.7 10.3L9.9 7.1C10.3 6.7 10.3 6.1 9.9 5.7L9.9 5.7C9.5 5.3 8.9 5.3 8.5 5.7L7 7.3 7 7.3ZM0 13C0 12.4 0.5 12 1 12L11 12C11.6 12 12 12.4 12 13 12 13.6 11.5 14 11 14L1 14C0.4 14 0 13.6 0 13L0 13Z"/></g></g></g></svg>
            <span class="hide-text-small">Download</span>
            <span>PDF</span>
        </a>

                                                    </div>


                                                <div class="citations" data-component="SV.Dropdown" data-namespace="citations">
        <h3 class="u-h4" data-role="button-dropdown__title">
    <span>Export</span>
    <span class="hide-text-small">citation</span>
</h3>
<ul class="citations__content" data-role="button-dropdown__content">
        <li>
            <a href="//citation-needed.springer.com/v2/references/10.1007/s10606-015-9218-5?format&#x3D;refman&amp;flavour&#x3D;citation"
               title="Download this article&#39;s citation as a .RIS file" class="gtm-export-citation" data-gtmlabel="RIS">
                <span class="citations__extension" data-gtmlabel="RIS">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"/></svg>
                    .RIS
                </span>
                <span class="citations__types">
                        <span>
                            Papers
                        </span>
                        <span>
                            Reference Manager
                        </span>
                        <span>
                            RefWorks
                        </span>
                        <span>
                            Zotero
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="//citation-needed.springer.com/v2/references/10.1007/s10606-015-9218-5?format&#x3D;endnote&amp;flavour&#x3D;citation"
               title="Download this article&#39;s citation as a .ENW file" class="gtm-export-citation" data-gtmlabel="ENW">
                <span class="citations__extension" data-gtmlabel="ENW">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"/></svg>
                    .ENW
                </span>
                <span class="citations__types">
                        <span>
                            EndNote
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="//citation-needed.springer.com/v2/references/10.1007/s10606-015-9218-5?format&#x3D;bibtex&amp;flavour&#x3D;citation"
               title="Download this article&#39;s citation as a .BIB file" class="gtm-export-citation" data-gtmlabel="BIB">
                <span class="citations__extension" data-gtmlabel="BIB">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"/></svg>
                    .BIB
                </span>
                <span class="citations__types">
                        <span>
                            BibTeX
                        </span>
                        <span>
                            JabRef
                        </span>
                        <span>
                            Mendeley
                        </span>
                </span>
            </a>
        </li>
</ul>
    </div>

                                                <div class="share-this" data-component="SV.Dropdown" data-namespace="shareThis">
        <h3 class="u-h4" data-role="button-dropdown__title">
    <span>Share</span>
    <span class="hide-text-small">article</span>
</h3>
<ul class="share-this__content" data-role="button-dropdown__content">
    <li>
        <a class="test-shareby-email-link gtm-shareby-email-link" href="mailto:?to&#x3D;&amp;subject&#x3D;Read%20this%20article%20on%20SpringerLink&amp;body&#x3D;Towards%20Concurrent%20Multi-Tasking%20in%20Shareable%20Interfaces%0A%0Ahttps%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10606-015-9218-5" title="Share this article via email">
            <span class="share-this__types">
                <svg width="24" height="24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none"><path fill-opacity="0" fill="#fff" d="M0 0h24v24h-24z"/><g transform="translate(3 5)"><rect fill="#F9F9F9" width="18" height="14" rx="2"/><path d="M1 2.006v9.988c0 .557.446 1.006.995 1.006h14.01c.549 0 .995-.449.995-1.006v-9.988c0-.557-.446-1.006-.995-1.006h-14.01c-.549 0-.995.449-.995 1.006zm-1 0c0-1.108.893-2.006 1.995-2.006h14.01c1.102 0 1.995.897 1.995 2.006v9.988c0 1.108-.893 2.006-1.995 2.006h-14.01c-1.102 0-1.995-.897-1.995-2.006v-9.988zM9 9l7-4v-1.443l-7 4-7-4v1.443z" fill="#666"/></g></g></svg>
                <span>Email</span>
            </span>
        </a>
    </li>
    <li>
        <a class="test-shareby-facebook-link gtm-shareby-facebook-link" href="https://www.facebook.com/sharer/sharer.php?u&#x3D;https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10606-015-9218-5" target="_blank" rel="noopener noreferrer" title="Share this article via Facebook">
            <span class="share-this__types">
                <svg width="24" height="24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none"><path fill-opacity="0" fill="#fff" d="M0 0h24v24h-24z"/><path d="M12.717 19.091h-2.66v-6.274h-1.329v-2.162h1.329v-1.298c0-1.763.75-2.813 2.883-2.813h1.775v2.162h-1.11c-.83 0-.885.302-.885.866l-.004 1.082h2.011l-.235 2.162h-1.775v6.274z" fill="#666"/></g></svg>
                <span>Facebook</span>
            </span>
        </a>
    </li>
    <li>
        <a class="test-shareby-twitter-link gtm-shareby-twitter-link" href="https://twitter.com/intent/tweet?text&#x3D;I%27m%20reading%20this%20on%20%23springerlink&amp;url&#x3D;https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10606-015-9218-5" target="_blank" rel="noopener noreferrer" title="Share this article via Twitter">
            <span class="share-this__types">
                <svg width="24" height="24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none"><path fill-opacity="0" fill="#fff" d="M0 0h24v24h-24z"/><path d="M16.651 9.189c.485-.306.858-.792 1.033-1.371-.454.284-.957.49-1.493.601-.428-.482-1.039-.783-1.715-.783-1.298 0-2.349 1.11-2.349 2.478 0 .194.019.384.06.564-1.952-.104-3.684-1.089-4.843-2.59-.202.367-.318.793-.318 1.247 0 .859.415 1.618 1.045 2.063-.385-.013-.748-.126-1.065-.31v.03c0 1.201.809 2.203 1.886 2.43-.198.058-.405.087-.62.087-.151 0-.299-.015-.442-.044.299.984 1.166 1.702 2.195 1.721-.805.665-1.818 1.061-2.919 1.061-.19 0-.377-.011-.561-.034 1.04.703 2.275 1.113 3.602 1.113 4.323 0 6.686-3.777 6.686-7.052l-.006-.321c.459-.35.859-.786 1.173-1.283-.422.197-.875.33-1.349.39z" fill="#666"/></g></svg>
                <span>Twitter</span>
            </span>
        </a>
    </li>
        <li>
            <a class="test-shareby-linkedin-link gtm-shareby-linkedin-link" href="https://www.linkedin.com/shareArticle?mini&#x3D;true&amp;url&#x3D;https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10606-015-9218-5&amp;title&#x3D;Towards Concurrent Multi-Tasking in Shareable Interfaces&amp;summary&#x3D;Shareable interfaces, those that can be interacted simultaneously by several users, are a common tool used both in CSCW research and in real world applications. They tend however to lack a capability that has been traditionally relevant to the usefu…" target="_blank" rel="noopener noreferrer" title="Share this article via LinkedIn">
                <span class="share-this__types">
                    <svg width="24" height="24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none"><path fill-opacity="0" fill="#fff" d="M0 0h24v24h-24z"/><path d="M6.821 10.044h2.126v7.41h-2.126v-7.41zm1.009-.927h-.015c-.77 0-1.269-.566-1.269-1.284 0-.732.514-1.287 1.299-1.287.784 0 1.267.554 1.282 1.285 0 .717-.498 1.286-1.297 1.286zm9.625 8.338h-2.411v-3.835c0-1.004-.377-1.688-1.206-1.688-.634 0-.987.462-1.151.908-.062.159-.052.382-.052.606v4.01h-2.389s.031-6.794 0-7.411h2.389v1.163c.141-.509.904-1.234 2.122-1.234 1.511 0 2.698 1.067 2.698 3.361v4.121z" fill="#666"/></g></svg>
                    <span>LinkedIn</span>
                </span>
            </a>
        </li>
        <li>
            <a class="gtm-shareby-sharelink-link" data-test="shareable-link" href="/sharelink/10.1007/s10606-015-9218-5" target="_blank" rel="noopener noreferrer" title="Get shareable link">
                <span class="share-this__types">
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="#666" d="M9 7c-2.8 0-5 2.2-5 5s2.2 5 5 5h2v-2h-2c-1.7 0-3-1.3-3-3s1.3-3 3-3h2v-2h-2zm6 0c2.8 0 5 2.2 5 5s-2.2 5-5 5h-2v-2h2c1.7 0 3-1.3 3-3s-1.3-3-3-3h-2v-2h2zm-1 4c.5 0 1 .4 1 1s-.5 1-1 1h-4c-.5 0-1-.4-1-1s.5-1 1-1h4z"/></svg>
                    <span>Shareable link</span>
                </span>
            </a>
        </li>
</ul>
    </div>

                                        </div>
                                    </div>
                                    <nav class="toc" aria-label="article contents">
    <h2 class="u-h4 u-screenreader-only">Table of contents</h2>
    <ul id="article-contents" class="article-contents" role="menu">
            <li role="menuitem">
                <a title="Article" href="#enumeration"><span class="u-overflow-ellipsis">Article</span></a>
            </li>
            <li role="menuitem">
                <a title="Abstract" href="#Abs1"><span class="u-overflow-ellipsis">Abstract</span></a>
            </li>
            <li role="menuitem">
                <a title="1 Introduction" href="#Sec1"><span class="u-overflow-ellipsis">1 Introduction</span></a>
            </li>
            <li role="menuitem">
                <a title="2 From personal to collaborative computing" href="#Sec2"><span class="u-overflow-ellipsis">2 From personal to collaborative computing</span></a>
            </li>
            <li role="menuitem">
                <a title="3 Multi-tasking in shareable interfaces: current situation and related research" href="#Sec6"><span class="u-overflow-ellipsis">3 Multi-tasking in shareable interfaces: current situation and related research</span></a>
            </li>
            <li role="menuitem">
                <a title="4 Approaches to multi-tasking" href="#Sec7"><span class="u-overflow-ellipsis">4 Approaches to multi-tasking</span></a>
            </li>
            <li role="menuitem">
                <a title="5 Implementation of GestureAgents framework" href="#Sec12"><span class="u-overflow-ellipsis">5 Implementation of GestureAgents framework</span></a>
            </li>
            <li role="menuitem">
                <a title="6 Example applications and systems created with GestureAgents" href="#Sec18"><span class="u-overflow-ellipsis">6 Example applications and systems created with GestureAgents</span></a>
            </li>
            <li role="menuitem">
                <a title="7 Discussion" href="#Sec19"><span class="u-overflow-ellipsis">7 Discussion</span></a>
            </li>
            <li role="menuitem">
                <a title="8 Conclusions" href="#Sec20"><span class="u-overflow-ellipsis">8 Conclusions</span></a>
            </li>
            <li role="menuitem">
                <a title="Acknowledgments" href="#Acknowledgments"><span class="u-overflow-ellipsis">Acknowledgments</span></a>
            </li>
            <li role="menuitem">
                <a title="References" href="#Bib1"><span class="u-overflow-ellipsis">References</span></a>
            </li>
            <li role="menuitem">
                <a title="Copyright information" href="#copyrightInformation"><span class="u-overflow-ellipsis">Copyright information</span></a>
            </li>
            <li role="menuitem">
                <a title="Authors and Affiliations" href="#authorsandaffiliations"><span class="u-overflow-ellipsis">Authors and Affiliations</span></a>
            </li>
            <li role="menuitem">
                <a title="About this article" href="#aboutcontent"><span class="u-overflow-ellipsis">About this article</span></a>
            </li>
    </ul>
</nav>

                                </div>
                                        <div class="skyscraper-ad u-hide" data-component="SpringerLink.GoogleAds" data-namespace="skyscraper"></div>

                            </div>
                        </aside>
                    </div>
                </article>
            </main>

            <section class="banner banner--cookies-policy u-js-hide" data-component="SpringerLink.CookiePolicy">
    <div class="banner__content u-interface">
        <h2 class="u-screenreader-only">Cookies</h2>
        <span class="banner__cookie-text">We use cookies to improve your experience with our site.</span>
        <a class="banner__link banner__link--cookie" href="/termsandconditions#cookies">More information</a>

        <form class="banner__form" action="/acceptcookies" method="POST">
            <button class="u-link-like banner__right-ui">Accept</button>
        </form>
    </div>
</section>

                <footer class="footer u-interface">
        <div class="footer__aside-wrapper">
            <div class="footer__content">
                <div class="footer__aside">
    <p class="footer__strapline">Over 10 million scientific documents at your fingertips</p>
                <div class="footer__edition" data-component="SpringerLink.EditionSwitcher">
                    <h3 class="u-hide" data-role="button-dropdown__title" data-btn-text="Switch between Academic &#38; Corporate Edition">Switch Edition</h3>
                    <ul data-role="button-dropdown__content">
                        <li class="selected"><a href="/siteEdition/link" id="siteedition-academic-link">Academic Edition</a></li>
                        <li><a href="/siteEdition/rd" id="siteedition-corporate-link">Corporate Edition</a></li>
                    </ul>
                </div>
</div>
            </div>
        </div>
        <div class="footer__content">
            <ul class="footer__nav">
    <li>
        <a href="/">Home</a>
    </li>
    <li>
        <a href="/impressum">Impressum</a>
    </li>
    <li>
        <a href="/termsandconditions">Legal Information</a>
    </li>
    <li>
        <a href="/accessibility" class="gtm-footer-accessibility">Accessibility</a>
    </li>
    <li>
        <a id="contactus-footer-link" href="/contactus">Contact Us</a>
    </li>
</ul>
            <a class="parent-logo"
   target="_blank" rel="noreferrer noopener"
   href="//www.springernature.com"
   title="Go to Springer Nature">
    <span class="u-screenreader-only">Springer Nature</span>
    <svg width="125" height="12">
        <image width="125" height="12" alt="Springer Nature logo"
               src="/springerlink-static/373808856/images/png/springernature.png"
               xmlns:xlink="http://www.w3.org/1999/xlink"
               xlink:href="/springerlink-static/373808856/images/svg/springernature.svg">
        </image>
    </svg>
</a>

<p class="footer__copyright">&copy; 2017 Springer International Publishing AG. Part of <a target="_blank" rel="noreferrer noopener" href="//www.springernature.com">Springer Nature</a>.</p>

    <p class="footer__user-access-info">
        <span>Not logged in</span>
        <span>Carnegie Mellon University Hunt Library (1600047252) - Carnegie Mellon University Hunt Library (3000133174) - Center for Research LIbraries c/o NERL (8200828607)</span>
        <span>128.237.216.95</span>
    </p>

        </div>
    </footer>

        </div>
        
<script type="text/javascript">
    (function() {
        var linkEl = document.querySelector('.js-ctm');
        if (window.matchMedia && window.matchMedia(linkEl.media)) {
            (function(h){h.className = h.className.replace('no-js', 'js')})(document.documentElement);
            window.SpringerLink = window.SpringerLink || {};
            window.SpringerLink.staticLocation = '/springerlink-static/373808856';
            var scriptJquery = document.createElement('script');
            var scriptMain = document.createElement('script');
            scriptJquery.async = false;
            scriptJquery.src = window.SpringerLink.staticLocation + '/js/jquery-3.2.1.min.js';
            scriptMain.async = false;
            scriptMain.src =  window.SpringerLink.staticLocation + '/js/main.js';
            document.body.appendChild(scriptJquery);
            document.body.appendChild(scriptMain);
        }
    })();
</script>

<script type="text/javascript">
    window.onload = function() {
    var linkEl = document.querySelector('.js-ctm');



    function viewport(size) {
        if (document.documentElement.clientWidth < 620) {
            size = 'small';
        }
        else if(document.documentElement.clientWidth < 1075 ) {
            size = 'medium';
        }
        else {
            size = 'wide';
        }
        return size
    }

    function reportForMouseEvent(linkCssSelector, nolardUrl, experiment, abgroup) {
        $('body').delegate(linkCssSelector, 'click', function() {
            reportConversion(nolardUrl, experiment, abgroup);
        });
    }

    function reportForMouseEventOnce(linkCssSelector, nolardUrl, experiment, abgroup) {
        var counter = 0;
        $('body').delegate(linkCssSelector, 'click', function() {
            if(counter == 0) {
                reportConversion(nolardUrl, experiment, abgroup);
                counter++;
            }
        });
    }

    function reportParticipation(nolardUrl, experiment, abgroup) {
        $.ajax({ url: nolardUrl + '/participate/' + experiment + '/' + abgroup });
    }

    function reportConversion(nolardUrl, experiment, abgroup) {
        $.ajax({ url: nolardUrl + '/convert/' + experiment + '/' + abgroup });
    }
    };
</script>

<script class="kxct" data-id="KDqyaFZ_" data-timing="async" data-version="3.0" type="text/javascript">
 window.Krux||((Krux=function(){Krux.q.push(arguments)}).q=[]);
 (function(){
   var k=document.createElement('script');k.type='text/javascript';k.async=true;
   k.src=(location.protocol==='https:'?'https:':'http:')+'//cdn.krxd.net/controltag/KDqyaFZ_.js';
   var s=document.getElementsByTagName('script')[0];s.parentNode.insertBefore(k,s);
 }());
</script>

<script>
    window.Krux||((Krux=function(){Krux.q.push(arguments);}).q=[]);
    (function(){
        function retrieve(n){
            var m, k='kx'+n;
            if (window.localStorage) {
                return window.localStorage[k] || "";
            } else if (navigator.cookieEnabled) {
                m = document.cookie.match(k+'=([^;]*)');
                return (m && unescape(m[1])) || "";
            } else {
                return '';
            }
        }
        Krux.user = retrieve('user');
        Krux.segments = retrieve('segs') && retrieve('segs').split(',') || [];
    })();
</script>

    <script type="text/javascript">
        var googletag = googletag || {};
        googletag.cmd = googletag.cmd || [];
        (function() {
            var gads = document.createElement('script');
            gads.async = true;
            gads.type = 'text/javascript';
            var useSSL = 'https:' == document.location.protocol;
            gads.src = (useSSL ? 'https:' : 'http:') +
                    '//www.googletagservices.com/tag/js/gpt.js';
            var node = document.getElementsByTagName('script')[0];
            node.parentNode.insertBefore(gads, node);
        })();
    </script>
    <script type="text/javascript" id="googletag-push">
        
            var adSlot = '6313/casper/journal/article';
        

        var definedSlots = [
                {slot: [728, 90], containerName: 'doubleclick-leaderboard-ad'},
                {slot: [160, 600], containerName: 'doubleclick-ad'},
        ];

        googletag.cmd.push(function() {
                googletag.pubads().setTargeting("doi","10.1007-s10606-015-9218-5");
                googletag.pubads().setTargeting("kwrd",["Concurrent_interaction","Multi-user","Shareable_interfaces","Multi-tasking","Agent_exclusivity"]);
                googletag.pubads().setTargeting("pmc",["I","I00001","I18067","Y00007","V23000","X00000"]);
                googletag.pubads().setTargeting("BPID",["1600047252","3000133174","8200828607"]);
                googletag.pubads().setTargeting("edition","academic");
                googletag.pubads().setTargeting("sucode","SC6");
                googletag.pubads().setTargeting("eissn","1573-7551");
                googletag.pubads().setTargeting("pissn","0925-9724");
                googletag.pubads().setTargeting("ksg",Krux.segments);
                googletag.pubads().setTargeting("kuid",Krux.uid);
                googletag.pubads().setTargeting("logged","N");
            googletag.pubads().enableSingleRequest();
            googletag.pubads().enableAsyncRendering();
            googletag.enableServices();
        });
    </script>

        
        <span id="chat-widget" class="u-hide"></span>
                    <noscript>
                <img aria-hidden="true" role="presentation" src="https://ssl-springer.met.vgwort.de/na/pw-vgzm.415900-10.1007-s10606-015-9218-5" width='1' height='1' alt='' />
            </noscript>

    </body>
</html>
